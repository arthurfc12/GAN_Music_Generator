{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating music with GTZAN and Keras\n",
    "Let's generate some music! A lot of techniques I've seen around use symbolic methods (generate tabs / midi), but I think giving the model full control over the waveform is a lot more challenging and potentially a lot more fun / cooler. The model architecture I'll be using is very inspired by [this awesome example in the keras docs](https://keras.io/examples/generative/ddim/), which I highly recommend taking a look at if you want to learn more about diffusion models\n",
    "\n",
    "Let's start off by pinning our version of librosa so that our logs look reasonable (skip this step at your own risk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation of technique\n",
    "Skip this part if you don't like reading ðŸ˜†. The overall idea here is to generate music directly from spectrograms (or some kind of 2d representation of the audio data) and then convert the spectrogram back to audio.\n",
    "\n",
    "### Feature representation\n",
    "I consider this to be the crux of this whole technique. Because image generation with CNNs is very popular and has been super successful recently (especially with the explosive progress made recently with diffusion models). The way I use the audio data is by using a [Modified Discrete Cosine Transform (MDCT)](https://www.tensorflow.org/api_docs/python/tf/signal/mdct). Using this particular transform is useful because you can exactly reconstruct your audio waveform from the MDCT spectrogram by applying the inverse MDCT transform. It's also useful because, unlike the fourier transform, all of the values that come from the MDCT transform are real valued. That way, you don't have to fool around with trying to reconstruct the phase of your new signal. This is what allows us to take advantage of 2d convolutional neural networks in order to create a 2d spectrogram and convert it directly to audio.  \n",
    "\n",
    "### Model Architecture\n",
    "I chose U-net, mostly because it seems to be the standard choice for training Diffusion models. I add attention in the bottom upsampling layer. Would be cool to try adding attention to more parts of this model and see how it affects the generated examples. I've intended to, but tend to OOM the GPU when I do. Happy to hear suggestions.\n",
    "\n",
    "### Training & Sampling\n",
    "In my own personal experiments, I've tried a few different generative models for this task. This includes several types of GAN and VAE models. It seems like diffusion works (by far) much better. I (once again) borrow [this implementation of Denoising Diffusion Implicit Models](https://keras.io/examples/generative/ddim/) (seriously, take a read if you're interested. I find it to be very practical and digestable). I pretty arbitrarily choose 1000 diffusion steps for sampling. Maybe more is better? Maybe less? Worth playing with more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uninstalling CPU-only tensorflow-intel...\n",
      "\n",
      "Installing TensorFlow 2.13 with GPU support...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.10.10 environment at: venv\u001b[0m\n",
      "\u001b[2mUninstalled \u001b[1m2 packages\u001b[0m \u001b[2min 14.79s\u001b[0m\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mtensorflow\u001b[0m\u001b[2m==2.11.0\u001b[0m\n",
      " \u001b[31m-\u001b[39m \u001b[1mtensorflow-intel\u001b[0m\u001b[2m==2.11.0\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Installation complete! Please restart the kernel.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[31merror\u001b[39m\u001b[0m: Failed to parse: `'tensorflow[and-cuda]==2.13.0'`\n",
      "  \u001b[1m\u001b[31mCaused by\u001b[39m\u001b[0m: Expected package name starting with an alphanumeric character, found `'`\n",
      "'tensorflow[and-cuda]==2.13.0'\n",
      "^\n"
     ]
    }
   ],
   "source": [
    "# Uninstall CPU-only tensorflow-intel and install TensorFlow with GPU support\n",
    "# TensorFlow 2.11 doesn't support [and-cuda] extra, so we'll upgrade to 2.13+ which does\n",
    "print(\"Uninstalling CPU-only tensorflow-intel...\")\n",
    "!uv pip uninstall tensorflow-intel tensorflow\n",
    "\n",
    "print(\"\\nInstalling TensorFlow 2.13 with GPU support...\")\n",
    "# For TensorFlow 2.13+, [and-cuda] automatically installs CUDA and cuDNN on Windows\n",
    "!uv pip install 'tensorflow[and-cuda]==2.13.0' tensorflow-addons\n",
    "\n",
    "print(\"\\nâœ… Installation complete! Please restart the kernel.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe file 'venv\\lib\\site-packages\\typing_extensions.py' seems to be overriding built in modules and interfering with the startup of the kernel. Consider renaming the file and starting the kernel again.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresOverridingBuiltInModules'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "!uv pip install \"tensorflow[and-cuda]==2.13.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU Setup Instructions\n",
    "\n",
    "**Important:** After installing TensorFlow with GPU support in the cell above:\n",
    "1. **Restart the kernel** (Kernel â†’ Restart Kernel)\n",
    "2. Then run the import cell below to configure GPU\n",
    "\n",
    "**Note:** The installation will:\n",
    "- Remove the CPU-only `tensorflow-intel` package\n",
    "- Install TensorFlow 2.15+ which supports GPU via the `[and-cuda]` extra\n",
    "- Automatically download compatible CUDA and cuDNN libraries\n",
    "\n",
    "**If you prefer to install CUDA manually**, you can also install just `tensorflow` (without the version constraint) and separately install CUDA 11.8 or 12.x and cuDNN.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-11-16T01:15:13.273434Z",
     "iopub.status.busy": "2022-11-16T01:15:13.27307Z",
     "iopub.status.idle": "2022-11-16T01:15:31.343817Z",
     "shell.execute_reply": "2022-11-16T01:15:31.342479Z",
     "shell.execute_reply.started": "2022-11-16T01:15:13.273402Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe file 'venv\\lib\\site-packages\\typing_extensions.py' seems to be overriding built in modules and interfering with the startup of the kernel. Consider renaming the file and starting the kernel again.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresOverridingBuiltInModules'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import librosa \n",
    "from glob import glob\n",
    "import google.protobuf\n",
    "\n",
    "import random\n",
    "from functools import partial\n",
    "import warnings\n",
    "import IPython.display as ipd\n",
    "from tensorflow.keras import mixed_precision\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure GPU memory growth to avoid allocating all GPU memory at once\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Enable memory growth for all GPUs (use newer API if available)\n",
    "        for gpu in gpus:\n",
    "            try:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            except AttributeError:\n",
    "                # For newer TensorFlow versions, use the non-experimental API\n",
    "                tf.config.set_memory_growth(gpu, True)\n",
    "        print(f\"âœ… Found {len(gpus)} GPU(s). Memory growth enabled.\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"GPU configuration error: {e}\")\n",
    "else:\n",
    "    print(\"âš ï¸  No GPU detected. TensorFlow will use CPU.\")\n",
    "    print(\"   Install TensorFlow with GPU: !uv pip install 'tensorflow[and-cuda]'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-16T01:15:31.349225Z",
     "iopub.status.busy": "2022-11-16T01:15:31.34846Z",
     "iopub.status.idle": "2022-11-16T01:15:31.552875Z",
     "shell.execute_reply": "2022-11-16T01:15:31.551799Z",
     "shell.execute_reply.started": "2022-11-16T01:15:31.349187Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check available devices\n",
    "print(\"Available physical devices:\")\n",
    "for device in tf.config.list_physical_devices():\n",
    "    print(f\"  - {device}\")\n",
    "\n",
    "# Check if GPU is available\n",
    "if tf.config.list_physical_devices('GPU'):\n",
    "    print(\"\\nâœ… GPU is available!\")\n",
    "    print(f\"GPU device: {tf.config.list_physical_devices('GPU')[0]}\")\n",
    "    # Print GPU details\n",
    "    gpu_details = tf.config.list_physical_devices('GPU')[0]\n",
    "    print(f\"GPU details: {gpu_details}\")\n",
    "else:\n",
    "    print(\"\\nâŒ No GPU detected. To enable GPU support:\")\n",
    "    print(\"  1. Install TensorFlow with GPU: !uv pip install 'tensorflow[and-cuda]'\")\n",
    "    print(\"  2. Make sure you have a compatible NVIDIA GPU\")\n",
    "    print(\"  3. Restart the kernel after installation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pick some files to train on\n",
    "\n",
    "I've found that this doesn't work so well if you try to train the model on all genres at once. So let's pick a single genre. I'll pick classical for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-16T01:15:31.556211Z",
     "iopub.status.busy": "2022-11-16T01:15:31.555802Z",
     "iopub.status.idle": "2022-11-16T01:15:31.594002Z",
     "shell.execute_reply": "2022-11-16T01:15:31.593077Z",
     "shell.execute_reply.started": "2022-11-16T01:15:31.556144Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "music_files = glob(\"/data/REAL_audio/jazz.*.wav\")\n",
    "music_files[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define keras layers\n",
    "Define the layers we'll use to build our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2022-11-16T01:15:31.595641Z",
     "iopub.status.busy": "2022-11-16T01:15:31.595353Z",
     "iopub.status.idle": "2022-11-16T01:15:31.614156Z",
     "shell.execute_reply": "2022-11-16T01:15:31.613197Z",
     "shell.execute_reply.started": "2022-11-16T01:15:31.595617Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def sinusoidal_embedding(x):\n",
    "    embedding_min_frequency = 1.0\n",
    "    embedding_max_frequency = 1000.0\n",
    "    embedding_dims = 32\n",
    "    frequencies = tf.exp(\n",
    "        tf.linspace(\n",
    "            tf.math.log(embedding_min_frequency),\n",
    "            tf.math.log(embedding_max_frequency),\n",
    "            embedding_dims // 2,\n",
    "        )\n",
    "    )\n",
    "    angular_speeds = 2.0 * math.pi * frequencies\n",
    "    embeddings = tf.concat(\n",
    "        [tf.sin(angular_speeds * x), tf.cos(angular_speeds * x)], axis=3\n",
    "    )\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def ResidualBlock(width):\n",
    "    def apply(x):\n",
    "        input_width = x.shape[3]\n",
    "        if input_width == width:\n",
    "            residual = x\n",
    "        else:\n",
    "            residual = layers.Conv2D(width, kernel_size=1)(x)\n",
    "        x = layers.BatchNormalization(center=False, scale=False)(x)\n",
    "        x = layers.Conv2D(\n",
    "            width, kernel_size=3, padding=\"same\", activation=keras.activations.swish\n",
    "        )(x)\n",
    "        x = layers.Conv2D(width, kernel_size=3, padding=\"same\")(x)\n",
    "        x = layers.Add()([x, residual])\n",
    "        return x\n",
    "\n",
    "    return apply\n",
    "\n",
    "\n",
    "def DownBlock(width, block_depth):\n",
    "    def apply(x):\n",
    "        x, skips = x\n",
    "        for _ in range(block_depth):\n",
    "            x = ResidualBlock(width)(x)\n",
    "            skips.append(x)\n",
    "        x = layers.AveragePooling2D(pool_size=2)(x)\n",
    "        return x\n",
    "\n",
    "    return apply\n",
    "\n",
    "\n",
    "def UpBlock(width, block_depth, attention=False):\n",
    "    def apply(x):\n",
    "        x, skips = x\n",
    "        x = layers.UpSampling2D(size=2, interpolation=\"bilinear\")(x)\n",
    "        for _ in range(block_depth):\n",
    "            skip = skips.pop()\n",
    "            x = layers.Concatenate()([x, skip] if not attention else [\n",
    "                x, skip, layers.MultiHeadAttention(\n",
    "                    num_heads=4, key_dim=1, attention_axes=(1,2)\n",
    "                )(x, skip)\n",
    "            ])\n",
    "            x = ResidualBlock(width)(x)\n",
    "        return x\n",
    "\n",
    "    return apply\n",
    "\n",
    "\n",
    "def get_network(widths, block_depth, dim1=256, dim2=128, channels=1, attention=False):\n",
    "    noisy_input = keras.Input(shape=(dim1, dim2, channels))\n",
    "    noise_variances = keras.Input(shape=(1, 1, 1))\n",
    "    \n",
    "    upsample_shape = (dim1, dim2)\n",
    "\n",
    "    e = layers.Lambda(sinusoidal_embedding)(noise_variances)\n",
    "    e = layers.UpSampling2D(size=upsample_shape, interpolation=\"nearest\")(e)\n",
    "\n",
    "    x = layers.Conv2D(widths[0], kernel_size=1)(noisy_input)\n",
    "    x = layers.Concatenate()([x, e])\n",
    "\n",
    "    skips = []\n",
    "    for width in widths[:-1]:\n",
    "        x = DownBlock(width, block_depth)([x, skips])\n",
    "\n",
    "    for _ in range(block_depth):\n",
    "        x = ResidualBlock(widths[-1])(x)\n",
    "\n",
    "    for idx, width in enumerate(reversed(widths[:-1])):\n",
    "        x = UpBlock(width, block_depth, attention=attention and idx ==0)([x, skips])\n",
    "\n",
    "    x = layers.Conv2D(channels, kernel_size=1, kernel_initializer=\"zeros\")(x)\n",
    "\n",
    "    return keras.Model([noisy_input, noise_variances], x, name=\"residual_unet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model\n",
    "Here, we'll build our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-16T01:18:05.746831Z",
     "iopub.status.busy": "2022-11-16T01:18:05.74615Z",
     "iopub.status.idle": "2022-11-16T01:18:05.784871Z",
     "shell.execute_reply": "2022-11-16T01:18:05.783822Z",
     "shell.execute_reply.started": "2022-11-16T01:18:05.746792Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "min_signal_rate = 0.02\n",
    "max_signal_rate = 0.95\n",
    "ema = 0.999\n",
    "\n",
    "def spectral_norm(pred, real):\n",
    "    \"\"\"Calculate difference in spectral norm between two batches of spectrograms.\"\"\"\n",
    "    norm_real = tf.norm(real, axis=(1,2)) + 1e-6\n",
    "    norm_pred = tf.norm(pred, axis=(1,2)) + 1e-6\n",
    "    return tf.reduce_mean(tf.abs(norm_real - norm_pred) / norm_real)\n",
    "\n",
    "def time_derivative(pred, real, window=1):\n",
    "    real_derivative = real[:, :-window, :, :] - real[:, window:, :, :]\n",
    "    pred_derivative = pred[:, :-window, :, :] - pred[:, window:, :, :]\n",
    "    return tf.reduce_mean(tf.keras.losses.MSE(real_derivative, pred_derivative))\n",
    "\n",
    "\n",
    "\n",
    "class DDIM(keras.Model):\n",
    "    \"\"\"DDIM model modified from this tutorial: https://keras.io/examples/generative/ddim/\"\"\"\n",
    "    \n",
    "    def __init__(self, widths, block_depth, attention=False, dim1=256, dim2=128):\n",
    "        super().__init__()\n",
    "\n",
    "        self.normalizer = layers.Normalization(axis=(2,3))\n",
    "        self.network = get_network(widths, block_depth, attention=attention, dim1=dim1, dim2=dim2)\n",
    "        self.ema_network = keras.models.clone_model(self.network)\n",
    "        self.spec_mod = 0\n",
    "        self.dx_mod = 0\n",
    "\n",
    "    def compile(self, **kwargs):\n",
    "        super().compile(**kwargs)\n",
    "\n",
    "        self.noise_loss_tracker = keras.metrics.Mean(name=\"n_loss\")\n",
    "        self.data_loss_tracker = keras.metrics.Mean(name=\"d_loss\")\n",
    "        \n",
    "        self.noise_spec_tracker = keras.metrics.Mean(name=\"n_spec\")\n",
    "        self.data_spec_tracker = keras.metrics.Mean(name=\"d_spec\")\n",
    "        \n",
    "        self.noise_dx_tracker = keras.metrics.Mean(name=\"n_dx\")\n",
    "        self.data_dx_tracker = keras.metrics.Mean(name=\"d_dx\")\n",
    "        \n",
    "        self.noise_total_tracker = keras.metrics.Mean(name=\"n_total\")\n",
    "        self.data_total_tracker = keras.metrics.Mean(name=\"d_total\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.noise_loss_tracker, \n",
    "            self.data_loss_tracker,\n",
    "            \n",
    "            self.noise_spec_tracker,\n",
    "            self.data_spec_tracker,\n",
    "            \n",
    "            self.noise_dx_tracker,\n",
    "            self.data_dx_tracker,\n",
    "            \n",
    "            self.noise_total_tracker,\n",
    "            self.data_total_tracker\n",
    "        ]\n",
    "    \n",
    "    def update_trackers(self, n_l, n_s, n_d, d_l, d_s, d_d):\n",
    "        \"\"\"Update all loss trackers.\"\"\"\n",
    "        n_t = n_l + n_s + n_d\n",
    "        d_t = d_l + d_s + d_d\n",
    "        \n",
    "        for loss, tracker in zip(\n",
    "            [n_l, n_s, n_d, n_t, d_l, d_s, d_d, d_t], \n",
    "            [\n",
    "                self.noise_loss_tracker, self.noise_spec_tracker, self.noise_dx_tracker, self.noise_total_tracker,\n",
    "                self.data_loss_tracker, self.data_spec_tracker, self.data_dx_tracker, self.data_total_tracker\n",
    "            ]\n",
    "        ):\n",
    "            tracker.update_state(loss)\n",
    "            \n",
    "    def get_losses(self, y_true, y_pred):\n",
    "        \"\"\"Get losses for model.\"\"\"\n",
    "        return (\n",
    "            tf.reduce_mean(\n",
    "                self.loss(y_pred, y_true)\n",
    "            ), spectral_norm(\n",
    "                y_pred, y_true\n",
    "            ), time_derivative(\n",
    "                y_pred, y_true\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def denormalize(self, data):\n",
    "        data = self.normalizer.mean + data * self.normalizer.variance**0.5\n",
    "        return tf.clip_by_value(data, -128.0, 128.0)\n",
    "\n",
    "    def diffusion_schedule(self, diffusion_times):\n",
    "        start_angle = tf.acos(max_signal_rate)\n",
    "        end_angle = tf.acos(min_signal_rate)\n",
    "        diffusion_angles = start_angle + diffusion_times * (end_angle - start_angle)\n",
    "        signal_rates = tf.cos(diffusion_angles)\n",
    "        noise_rates = tf.sin(diffusion_angles)\n",
    "        return noise_rates, signal_rates\n",
    "\n",
    "    def denoise(self, noisy_data, noise_rates, signal_rates, training):\n",
    "        if training:\n",
    "            network = self.network\n",
    "        else:\n",
    "            network = self.ema_network\n",
    "        pred_noises = network([noisy_data, noise_rates**2], training=training)\n",
    "        pred_data = (noisy_data - noise_rates * pred_noises) / signal_rates\n",
    "\n",
    "        return pred_noises, pred_data\n",
    "\n",
    "    def reverse_diffusion(self, initial_noise, diffusion_steps):\n",
    "        num_examples = tf.shape(initial_noise)[0]\n",
    "        step_size = 1.0 / diffusion_steps\n",
    "\n",
    "        # important line:\n",
    "        # at the first sampling step, the \"noisy data\" is pure noise\n",
    "        # but its signal rate is assumed to be nonzero (min_signal_rate)\n",
    "        next_noisy_data = initial_noise\n",
    "        for step in tqdm(range(diffusion_steps)):\n",
    "            noisy_data = next_noisy_data\n",
    "\n",
    "            # separate the current noisy data to its components\n",
    "            diffusion_times = tf.ones((num_examples, 1, 1, 1)) - step * step_size\n",
    "            noise_rates, signal_rates = self.diffusion_schedule(diffusion_times)\n",
    "            pred_noises, pred_data = self.denoise(\n",
    "                noisy_data, noise_rates, signal_rates, training=False\n",
    "            )\n",
    "            # network used in eval mode\n",
    "\n",
    "            # remix the predicted components using the next signal and noise rates\n",
    "            next_diffusion_times = diffusion_times - step_size\n",
    "            next_noise_rates, next_signal_rates = self.diffusion_schedule(\n",
    "                next_diffusion_times\n",
    "            )\n",
    "            next_noisy_data = (\n",
    "                next_signal_rates * pred_data + next_noise_rates * pred_noises\n",
    "            )\n",
    "            # this new noisy data will be used in the next step\n",
    "\n",
    "        return pred_data\n",
    "\n",
    "    def generate(self, num_examples, shape, diffusion_steps):\n",
    "        # noise -> data -> denormalized data\n",
    "        initial_noise = tf.random.normal(shape=(num_examples, shape[0], shape[1], shape[2]))\n",
    "        generated_data = self.reverse_diffusion(initial_noise, diffusion_steps)\n",
    "        generated_data = self.denormalize(generated_data)\n",
    "        return generated_data\n",
    "\n",
    "    def train_step(self, data):\n",
    "        batch_size = tf.shape(data)[0]\n",
    "        # normalize data to have standard deviation of 1, like the noises\n",
    "        data = self.normalizer(data, training=True)\n",
    "        noises = tf.random.normal(shape=tf.shape(data))\n",
    "\n",
    "        # sample uniform random diffusion times\n",
    "        diffusion_times = tf.random.uniform(\n",
    "            shape=(batch_size, 1, 1, 1), minval=0.0, maxval=1.0\n",
    "        )\n",
    "        noise_rates, signal_rates = self.diffusion_schedule(diffusion_times)\n",
    "        noise_rates = noise_rates\n",
    "        signal_rates = signal_rates\n",
    "        # mix the data with noises accordingly\n",
    "        noisy_data = signal_rates * data + noise_rates * noises\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # train the network to separate noisy data to their components\n",
    "            pred_noises, pred_data = self.denoise(\n",
    "                noisy_data, noise_rates, signal_rates, training=True\n",
    "            )\n",
    "\n",
    "            noise_loss, noise_spec, noise_dx = self.get_losses(noises, pred_noises) #safe_reduce_mean(self.loss(noises, pred_noises))  # used for training\n",
    "            total_noise_loss = tf.reduce_sum([\n",
    "                noise_loss, \n",
    "                self.spec_mod*noise_spec, \n",
    "                self.dx_mod*noise_dx\n",
    "            ])\n",
    "            data_loss, data_spec, data_dx = self.get_losses(data, pred_data) #safe_reduce_mean(self.loss(data, pred_data))  # only used as metric\n",
    "\n",
    "        gradients = tape.gradient(noise_loss, self.network.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.network.trainable_weights))\n",
    "\n",
    "        self.update_trackers(\n",
    "            noise_loss, noise_spec, noise_dx,\n",
    "            data_loss, data_spec, data_dx\n",
    "        )\n",
    "\n",
    "        # track the exponential moving averages of weights\n",
    "        for weight, ema_weight in zip(self.network.weights, self.ema_network.weights):\n",
    "            ema_weight.assign(ema * ema_weight + (1 - ema) * weight)\n",
    "\n",
    "        # KID is not measured during the training phase for computational efficiency\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    def test_step(self, data):\n",
    "        # normalize data to have standard deviation of 1, like the noises\n",
    "        batch_size = tf.shape(data)[0]\n",
    "        \n",
    "        data = self.normalizer(data, training=False)\n",
    "        noises = tf.random.normal(shape=tf.shape(data))\n",
    "\n",
    "        # sample uniform random diffusion times\n",
    "        diffusion_times = tf.random.uniform(\n",
    "            shape=(batch_size, 1, 1, 1), minval=0.0, maxval=1.0\n",
    "        )\n",
    "        noise_rates, signal_rates = self.diffusion_schedule(diffusion_times)\n",
    "        # mix the data with noises accordingly\n",
    "        noisy_data = signal_rates * data + noise_rates * noises\n",
    "\n",
    "        # use the network to separate noisy data to their components\n",
    "        pred_noises, pred_data = self.denoise(\n",
    "            noisy_data, noise_rates, signal_rates, training=False\n",
    "        )\n",
    "\n",
    "        noise_loss = self.loss(noises, pred_noises)\n",
    "        data_loss = self.loss(data, pred_data)\n",
    "\n",
    "        self.data_loss_tracker.update_state(data_loss)\n",
    "        self.noise_loss_tracker.update_state(noise_loss)\n",
    "\n",
    "        return {m.name: m.result() for m in self.metrics}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define data loader\n",
    "\n",
    "Here we're going to make some data loading utils and then define our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-16T01:16:33.713583Z",
     "iopub.status.busy": "2022-11-16T01:16:33.71309Z",
     "iopub.status.idle": "2022-11-16T01:16:33.729217Z",
     "shell.execute_reply": "2022-11-16T01:16:33.728229Z",
     "shell.execute_reply.started": "2022-11-16T01:16:33.713541Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_at_interval(x, rate=10_000, feats=256, duration=3.3):\n",
    "    \"\"\"Load music from file at some offset. Return MDCT spectrogram of that data\"\"\"\n",
    "    file = x[0].numpy().decode()\n",
    "    idx = x[1].numpy()\n",
    "    audio, sr = librosa.load(file, duration=duration, sr=rate, offset=idx)\n",
    "    audio_fill = np.zeros(int(rate*duration), dtype=np.float32)\n",
    "    audio_fill[:len(audio)] = audio\n",
    "    spec = tf.signal.mdct(audio_fill, feats)\n",
    "    return spec\n",
    "\n",
    "def load_audio(x,y, rate=10_000, mdct_feats=256, duration=3.3):\n",
    "    \"\"\"TF function for loading MDCT spectrogram from file.\"\"\"\n",
    "    out = tf.py_function(lambda x,y: load_at_interval( \n",
    "        (x,y), rate=rate, feats=mdct_feats, duration=duration\n",
    "    ), inp=[x,y], Tout=tf.float32)\n",
    "    return out\n",
    "\n",
    "def get_files_dataset(\n",
    "        glob_location,\n",
    "        total_seconds=2,\n",
    "        out_len = 3.3,\n",
    "        hop_size=1,\n",
    "        max_feats = 2048,\n",
    "        batch_size=4,\n",
    "        shuffer_size=1000,\n",
    "        scale=1,\n",
    "        rate=10_000,\n",
    "        mdct_feats=256\n",
    "    ):\n",
    "    \"\"\"Get file dataset loader for a glob of audio files.\"\"\"\n",
    "    \n",
    "    files = glob(\n",
    "        glob_location,\n",
    "        recursive=True\n",
    "    )\n",
    "    \n",
    "#     files = [file for file in files if file not in exclude]\n",
    "    \n",
    "    def file_list_generator():\n",
    "        for _ in range(total_seconds):\n",
    "            for file in files:\n",
    "                yield file, _*hop_size\n",
    "                \n",
    "    load_fn = partial(load_audio, duration=out_len, rate=rate, mdct_feats=mdct_feats)\n",
    "                \n",
    "    dg =tf.data.Dataset.from_generator(file_list_generator, output_signature = (\n",
    "        tf.TensorSpec(shape=(), dtype=tf.string), \n",
    "        tf.TensorSpec(shape=(), dtype=tf.int32))).shuffle(shuffer_size).map(\n",
    "            load_fn, num_parallel_calls=tf.data.AUTOTUNE\n",
    "        ).map(\n",
    "            lambda x: tf.expand_dims(x, -1)[:max_feats, :, :]*scale\n",
    "        ).map(\n",
    "            lambda x: tf.ensure_shape(x, (max_feats, mdct_feats//2, 1))\n",
    "        ).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    return dg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n arquivos: 1000\n",
      "['data\\\\REAL_audio\\\\blues.00000.wav', 'data\\\\REAL_audio\\\\blues.00001.wav', 'data\\\\REAL_audio\\\\blues.00002.wav']\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "DATA_DIR = \"data\\REAL_audio\"\n",
    "\n",
    "music_files = sorted(glob(os.path.join(DATA_DIR, \"**\", \"*.wav\"), recursive=True))\n",
    "print(\"n arquivos:\", len(music_files))\n",
    "print(music_files[:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-16T01:15:31.667285Z",
     "iopub.status.busy": "2022-11-16T01:15:31.666927Z",
     "iopub.status.idle": "2022-11-16T01:15:34.84944Z",
     "shell.execute_reply": "2022-11-16T01:15:34.848519Z",
     "shell.execute_reply.started": "2022-11-16T01:15:31.667251Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset = get_files_dataset(\n",
    "    \"data\\REAL_audio\\*.wav\", \n",
    "    out_len=3.3, \n",
    "    max_feats=256, \n",
    "    total_seconds=26, \n",
    "    scale=1,\n",
    "    batch_size=16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PrefetchDataset element_spec=TensorSpec(shape=(None, 256, 128, 1), dtype=tf.float32, name=None)>\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shape: (16, 256, 128, 1)\n"
     ]
    }
   ],
   "source": [
    "for batch in dataset.take(1):\n",
    "    print(\"Batch shape:\", batch.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-16T01:15:34.851047Z",
     "iopub.status.busy": "2022-11-16T01:15:34.850708Z",
     "iopub.status.idle": "2022-11-16T01:15:37.545969Z",
     "shell.execute_reply": "2022-11-16T01:15:37.544913Z",
     "shell.execute_reply.started": "2022-11-16T01:15:34.851011Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 256, 128, 1)\n"
     ]
    }
   ],
   "source": [
    "for test_batch in dataset.take(1):\n",
    "    shape = test_batch.shape\n",
    "\n",
    "print(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-16T01:15:37.553378Z",
     "iopub.status.busy": "2022-11-16T01:15:37.552956Z",
     "iopub.status.idle": "2022-11-16T01:15:37.56259Z",
     "shell.execute_reply": "2022-11-16T01:15:37.561438Z",
     "shell.execute_reply.started": "2022-11-16T01:15:37.553338Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "num_total_examples = (len(music_files) * 26) // shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-16T01:18:10.423756Z",
     "iopub.status.busy": "2022-11-16T01:18:10.423406Z",
     "iopub.status.idle": "2022-11-16T01:18:11.472002Z",
     "shell.execute_reply": "2022-11-16T01:18:11.471044Z",
     "shell.execute_reply.started": "2022-11-16T01:18:10.423727Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = DDIM(widths = [128, 128, 128, 128], block_depth = 2, \n",
    "             attention=True, dim1=shape[1], dim2=shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-16T01:18:11.474751Z",
     "iopub.status.busy": "2022-11-16T01:18:11.474372Z",
     "iopub.status.idle": "2022-11-16T01:18:26.628028Z",
     "shell.execute_reply": "2022-11-16T01:18:26.626005Z",
     "shell.execute_reply.started": "2022-11-16T01:18:11.474715Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model.normalizer.adapt(dataset, steps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-16T01:18:26.630117Z",
     "iopub.status.busy": "2022-11-16T01:18:26.629761Z",
     "iopub.status.idle": "2022-11-16T01:18:26.669783Z",
     "shell.execute_reply": "2022-11-16T01:18:26.66864Z",
     "shell.execute_reply.started": "2022-11-16T01:18:26.630081Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss=tf.keras.losses.MSE,\n",
    "    optimizer= tfa.optimizers.AdamW(\n",
    "        learning_rate = 3e-4,\n",
    "        weight_decay = 1e-4\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-16T01:16:56.646062Z",
     "iopub.status.busy": "2022-11-16T01:16:56.645695Z",
     "iopub.status.idle": "2022-11-16T01:16:56.65083Z",
     "shell.execute_reply": "2022-11-16T01:16:56.649709Z",
     "shell.execute_reply.started": "2022-11-16T01:16:56.646025Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset = dataset.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(fn, num_parallel_calls=tf.data.AUTOTUNE) \\\n",
    "                 .cache() \\\n",
    "                 .prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-16T01:18:26.676544Z",
     "iopub.status.busy": "2022-11-16T01:18:26.675818Z",
     "iopub.status.idle": "2022-11-16T01:24:15.05499Z",
     "shell.execute_reply": "2022-11-16T01:24:15.053726Z",
     "shell.execute_reply.started": "2022-11-16T01:18:26.676502Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1/1625 [..............................] - ETA: 27:40:36 - n_loss: 0.9997 - d_loss: 12.1665 - n_spec: 1.0000 - d_spec: 2.7377 - n_dx: 1.9989 - d_dx: 24.3451 - n_total: 3.9986 - d_total: 39.2492"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset.repeat(), steps_per_epoch=num_total_examples, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-16T01:24:15.057434Z",
     "iopub.status.busy": "2022-11-16T01:24:15.057039Z",
     "iopub.status.idle": "2022-11-16T01:24:15.065943Z",
     "shell.execute_reply": "2022-11-16T01:24:15.064338Z",
     "shell.execute_reply.started": "2022-11-16T01:24:15.057396Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model.spec_mod = 1\n",
    "model.dx_mod = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-16T01:24:15.067895Z",
     "iopub.status.busy": "2022-11-16T01:24:15.067411Z",
     "iopub.status.idle": "2022-11-16T03:13:52.740352Z",
     "shell.execute_reply": "2022-11-16T03:13:52.738307Z",
     "shell.execute_reply.started": "2022-11-16T01:24:15.06786Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(dataset.repeat(), steps_per_epoch = math.ceil(num_total_examples), epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate new samples using this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-16T03:16:21.802331Z",
     "iopub.status.busy": "2022-11-16T03:16:21.80196Z",
     "iopub.status.idle": "2022-11-16T03:18:57.307277Z",
     "shell.execute_reply": "2022-11-16T03:18:57.306225Z",
     "shell.execute_reply.started": "2022-11-16T03:16:21.802297Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "specs = model.generate(8, shape[1:], 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's look at some real examples first, to get an idea of what we're looking for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-16T03:14:16.688567Z",
     "iopub.status.busy": "2022-11-16T03:14:16.688193Z",
     "iopub.status.idle": "2022-11-16T03:14:18.08489Z",
     "shell.execute_reply": "2022-11-16T03:14:18.084024Z",
     "shell.execute_reply.started": "2022-11-16T03:14:16.688531Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    plt.pcolormesh(np.log(np.abs(test_batch[i, :, :, 0].numpy().T)))\n",
    "    plt.colorbar()\n",
    "    plt.title(f\"Real example {i+1}\")\n",
    "    plt.show()\n",
    "    ipd.display(ipd.Audio(tf.signal.inverse_mdct(test_batch[i, :, :, 0]), rate=10_000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's compare the generated samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-11-16T03:20:13.201071Z",
     "iopub.status.busy": "2022-11-16T03:20:13.200716Z",
     "iopub.status.idle": "2022-11-16T03:20:15.368053Z",
     "shell.execute_reply": "2022-11-16T03:20:15.367184Z",
     "shell.execute_reply.started": "2022-11-16T03:20:13.201041Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(specs)):\n",
    "    plt.pcolormesh(np.log(np.abs(specs[i, :, :, 0].numpy().T)))\n",
    "    plt.colorbar()\n",
    "    plt.title(f\"Generated example {i+1}\")\n",
    "    plt.show()\n",
    "    ipd.display(ipd.Audio(tf.signal.inverse_mdct(tf.cast(specs[i, :, :, 0], tf.float32)), rate=10_000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ideas for improvement\n",
    "\n",
    "Samples were interesting, but definitely far from great! There's a lot of room for improvement here. I've got a few ideas, but there are surely more. \n",
    "\n",
    "- Train for longer\n",
    "- Scale up the model\n",
    "- Scale up the dataset\n",
    "- Train on longer pieces of audio\n",
    "- Data augmentation (ex: phase inversion)\n",
    "- Weight the loss by perceived volume of frequencies (psychoacoustic features)? \n",
    "- Same thing, but using 1d signal instead? (not sure if this would actually help)\n",
    "- Play with sampling?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 568973,
     "sourceId": 1032238,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30213,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
