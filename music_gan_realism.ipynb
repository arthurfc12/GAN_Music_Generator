{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f155410e",
   "metadata": {},
   "source": [
    "\n",
    "# Music Realism Scoring with WGAN-GP (Log-Mel, Codec Parity)\n",
    "\n",
    "This notebook trains a **WGAN-GP on real music only** (using **log-mel spectrograms**) and then uses the **discriminator** as a **realism score** for new tracks (AI vs. real).  \n",
    "It handles **codec parity** (WAV ↔ MP3 round-trip) to prevent shortcut learning, and fixes input shapes to **[1, 128, 256]** (channels, mels, frames).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "605c88ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 2.9.0+cu128\n",
      "CUDA available: True\n",
      "Device count: 1\n",
      "GPU 0: NVIDIA GeForce RTX 4050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"PyTorch:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Device count:\", torch.cuda.device_count())\n",
    "    print(\"GPU 0:\", torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6376480b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob  # instead of: from glob import glob\n",
    "\n",
    "\n",
    "SEED = 17\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "REAL_AUDIO_DIR = \"data/REAL_audio\" # <-- only this is required now\n",
    "\n",
    "\n",
    "SR = 22050\n",
    "N_FFT = 1024\n",
    "HOP = 256\n",
    "WIN = 1024\n",
    "N_MELS = 128\n",
    "FMIN = 20\n",
    "FMAX = 8000\n",
    "\n",
    "\n",
    "# frames per training window (mel time steps)\n",
    "FRAMES = 256 # ≈ FRAMES*HOP/SR seconds (here ~2.97 s); raise if you have VRAM\n",
    "WINDOW_SEC = FRAMES * HOP / SR\n",
    "\n",
    "\n",
    "BATCH = 32\n",
    "EPOCHS = 50\n",
    "LR_G = 2e-4\n",
    "LR_D = 2e-4\n",
    "BETAS = (0.5, 0.9)\n",
    "LAMBDA_GP = 10.0\n",
    "N_CRITIC = 5 # D steps per G step\n",
    "\n",
    "\n",
    "SAVE_DIR = \"runs/real_only_gan\"\n",
    "SAMPLES_DIR = f\"{SAVE_DIR}/samples\"\n",
    "CKPT_DIR = f\"{SAVE_DIR}/ckpts\"\n",
    "os.makedirs(SAMPLES_DIR, exist_ok=True)\n",
    "os.makedirs(CKPT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "28cacb5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda | pyloudnorm: True | ffmpeg: True | sklearn: True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os, sys, math, random, shutil, tempfile, subprocess, warnings, glob\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import tqdm\n",
    "import torchaudio\n",
    "from torchaudio.transforms import MelSpectrogram, AmplitudeToDB\n",
    "\n",
    "# Optional deps\n",
    "try:\n",
    "    import pyloudnorm as pyln\n",
    "    HAVE_PYL = True\n",
    "except Exception:\n",
    "    HAVE_PYL = False\n",
    "\n",
    "try:\n",
    "    import soundfile as sf\n",
    "    HAVE_SF = True\n",
    "except Exception:\n",
    "    HAVE_SF = False\n",
    "\n",
    "\n",
    "# sklearn is optional for metrics\n",
    "try:\n",
    "    from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "    HAVE_SK = True\n",
    "except Exception:\n",
    "    HAVE_SK = False\n",
    "\n",
    "# Check FFmpeg availability (for MP3 round-trip)\n",
    "def _have_ffmpeg():\n",
    "    try:\n",
    "        subprocess.run([\"ffmpeg\", \"-version\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, check=False)\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "HAVE_FFMPEG = _have_ffmpeg()\n",
    "\n",
    "# Device selection\n",
    "if DEVICE == \"cuda\" and not torch.cuda.is_available():\n",
    "    DEVICE = \"cpu\"\n",
    "print(f\"Device: {DEVICE} | pyloudnorm: {HAVE_PYL} | ffmpeg: {HAVE_FFMPEG} | sklearn: {HAVE_SK}\")\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED); \n",
    "if DEVICE == \"cuda\":\n",
    "    torch.cuda.manual_seed_all(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cdf52738",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "\n",
    "try:\n",
    "    import soundfile as sf\n",
    "    HAVE_SF = True\n",
    "except Exception:\n",
    "    HAVE_SF = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cde5de03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pathlib import Path\n",
    "\n",
    "# from pathlib import Path\n",
    "# import numpy as np\n",
    "# import torch\n",
    "\n",
    "# def load_audio(path, sr=SR):\n",
    "#     \"\"\"\n",
    "#     Robust loader that avoids TorchCodec crashes:\n",
    "#     1) For WAV/AIFF/FLAC: try soundfile (libsndfile).\n",
    "#     2) For everything (incl. MP3/M4A): try librosa (audioread/ffmpeg).\n",
    "#     3) Last resort: torchaudio.load.\n",
    "#     Returns mono float tensor at target SR.\n",
    "#     \"\"\"\n",
    "#     ext = Path(path).suffix.lower()\n",
    "\n",
    "#     # 1) Prefer soundfile for lossless containers\n",
    "#     if HAVE_SF and ext in [\".wav\", \".aiff\", \".aif\", \".flac\", \".ogg\"]:\n",
    "#         try:\n",
    "#             y, file_sr = sf.read(str(path), dtype=\"float32\", always_2d=False)\n",
    "#             if y.ndim == 2:\n",
    "#                 y = y.mean(axis=1)\n",
    "#             wav_t = torch.from_numpy(y)\n",
    "#             if file_sr != SR:\n",
    "#                 wav_t = torchaudio.functional.resample(wav_t.unsqueeze(0), file_sr, SR).squeeze(0)\n",
    "#             return wav_t.contiguous()\n",
    "#         except Exception:\n",
    "#             pass\n",
    "\n",
    "#     # 2) Librosa for anything (mp3, m4a, wav, etc.)\n",
    "#     try:\n",
    "#         y, file_sr = librosa.load(str(path), sr=None, mono=True)\n",
    "#         y = y.astype(np.float32, copy=False)\n",
    "#         wav_t = torch.from_numpy(y)\n",
    "#         if file_sr != SR:\n",
    "#             wav_t = torchaudio.functional.resample(wav_t.unsqueeze(0), file_sr, SR).squeeze(0)\n",
    "#         return wav_t.contiguous()\n",
    "#     except Exception:\n",
    "#         pass\n",
    "\n",
    "#     # 3) Fallback to torchaudio (may require TorchCodec)\n",
    "#     wav, file_sr = torchaudio.load(str(path))  # [C, T]\n",
    "#     wav_t = wav.mean(dim=0)\n",
    "#     if file_sr != SR:\n",
    "#         wav_t = torchaudio.functional.resample(wav_t.unsqueeze(0), file_sr, SR).squeeze(0)\n",
    "#     return wav_t.contiguous()\n",
    "\n",
    "# def lufs_normalize(wav_t, sr=SR, target_lufs=TARGET_LUFS):\n",
    "#     if HAVE_PYL:\n",
    "#         y = wav_t.detach().cpu().numpy().astype(np.float32)\n",
    "#         meter = pyln.Meter(sr)\n",
    "#         try:\n",
    "#             loud = meter.integrated_loudness(y)\n",
    "#             gain_db = target_lufs - loud\n",
    "#             gain = 10 ** (gain_db / 20.0)\n",
    "#             y = np.clip(y * gain, -1.0, 1.0)\n",
    "#             return torch.from_numpy(y)\n",
    "#         except Exception:\n",
    "#             pass\n",
    "#     # Fallback: simple peak normalization\n",
    "#     peak = wav_t.abs().max().item()\n",
    "#     if peak > 0:\n",
    "#         wav_t = wav_t / peak\n",
    "#     return wav_t\n",
    "\n",
    "# def mp3_roundtrip(wav_t, sr=SR, bitrate=MP3_BITRATE):\n",
    "#     if not HAVE_FFMPEG:\n",
    "#         return wav_t\n",
    "#     try:\n",
    "#         with tempfile.TemporaryDirectory() as td:\n",
    "#             wav_path = os.path.join(td, \"tmp.wav\")\n",
    "#             mp3_path = os.path.join(td, \"tmp.mp3\")\n",
    "#             torchaudio.save(wav_path, wav_t.unsqueeze(0), sr)\n",
    "#             cmd = [\"ffmpeg\", \"-y\", \"-hide_banner\", \"-loglevel\", \"error\",\n",
    "#                    \"-i\", wav_path, \"-b:a\", bitrate, mp3_path]\n",
    "#             res = subprocess.run(cmd, check=False)\n",
    "#             if res.returncode != 0 or not os.path.exists(mp3_path):\n",
    "#                 # fall back gracefully\n",
    "#                 return wav_t\n",
    "#             wav2, sr2 = torchaudio.load(mp3_path)\n",
    "#             if sr2 != sr:\n",
    "#                 wav2 = torchaudio.functional.resample(wav2, sr2, sr)\n",
    "#             return torch.mean(wav2, dim=0)\n",
    "#     except Exception:\n",
    "#         return wav_t\n",
    "\n",
    "# # Log-mel transforms\n",
    "# _mel = MelSpectrogram(\n",
    "#     sample_rate=SR, n_fft=N_FFT, hop_length=HOP,\n",
    "#     n_mels=N_MELS, f_min=FMIN, f_max=FMAX, center=True, power=2.0\n",
    "# )\n",
    "# _to_db = AmplitudeToDB(stype=\"power\")\n",
    "\n",
    "# def to_logmel(wav_t):\n",
    "#     # wav_t: [T] float32/float64\n",
    "#     x = wav_t.unsqueeze(0)  # [1, T]\n",
    "#     mel = _mel(x)           # [1, mels, frames]\n",
    "#     mel_db = _to_db(mel)    # [1, mels, frames]\n",
    "#     # min-max scale to [-1, 1] per-sample\n",
    "#     m = mel_db.amin(dim=(1,2), keepdim=True)\n",
    "#     M = mel_db.amax(dim=(1,2), keepdim=True)\n",
    "#     mel_n = (mel_db - m) / (M - m + 1e-9)\n",
    "#     mel_n = mel_n * 2.0 - 1.0\n",
    "#     return mel_n   # [1, mels, frames] range ~ [-1,1]\n",
    "\n",
    "# def fix_frames(spec_1mT, frames=FRAMES):\n",
    "#     # spec_1mT: [1, mels, T]\n",
    "#     T = spec_1mT.shape[-1]\n",
    "#     if T == frames:\n",
    "#         return spec_1mT\n",
    "#     spec = F.interpolate(spec_1mT.unsqueeze(0), size=(N_MELS, frames), mode=\"bilinear\", align_corners=False)\n",
    "#     return spec.squeeze(0)  # [1, mels, frames]\n",
    "\n",
    "# def preprocess_file(path, codec_parity=False):\n",
    "#     wav = load_audio(path, sr=SR)\n",
    "#     if codec_parity:\n",
    "#         wav = mp3_roundtrip(wav, sr=SR)\n",
    "#     wav = lufs_normalize(wav, sr=SR, target_lufs=TARGET_LUFS)\n",
    "#     # Extract a center window of ~WIN_SECS before mel (or pad if short)\n",
    "#     N_SAMPLES = int(SR * WIN_SECS)\n",
    "#     if wav.numel() < N_SAMPLES:\n",
    "#         wav = F.pad(wav, (0, N_SAMPLES - wav.numel()))\n",
    "#     else:\n",
    "#         start = (wav.numel() - N_SAMPLES) // 2\n",
    "#         wav = wav[start:start+N_SAMPLES]\n",
    "#     mel = to_logmel(wav)          # [1, 128, T']\n",
    "#     mel = fix_frames(mel, FRAMES) # [1, 128, 256]\n",
    "#     return mel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1d12c663",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "\n",
    "EPS = 1e-7\n",
    "\n",
    "\n",
    "def wav_to_logmel(y: np.ndarray) -> np.ndarray:\n",
    "    S = librosa.feature.melspectrogram(\n",
    "        y=y, sr=SR, n_fft=N_FFT, hop_length=HOP, win_length=WIN,\n",
    "        n_mels=N_MELS, power=2.0, fmin=FMIN, fmax=FMAX,\n",
    "        )\n",
    "    S = np.maximum(S, EPS)\n",
    "    logS = np.log(S)\n",
    "    return logS # natural log of power mel\n",
    "\n",
    "\n",
    "def logmel_to_wav(logS: np.ndarray, length: int | None = None) -> np.ndarray:\n",
    "    S = np.exp(logS) # back to power mel\n",
    "    y = librosa.feature.inverse.mel_to_audio(\n",
    "        M=S, sr=SR, n_fft=N_FFT, hop_length=HOP, win_length=WIN,\n",
    "        fmin=FMIN, fmax=FMAX, power=2.0, n_iter=64\n",
    "        )\n",
    "    if length is not None and len(y) > length:\n",
    "        y = y[:length]\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f7a4664d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RealMelDataset(Dataset):\n",
    "    def __init__(self, root: str, frames: int = FRAMES):\n",
    "        exts = (\".wav\", \".flac\", \".mp3\", \".ogg\", \".m4a\", \".aiff\", \".aif\")\n",
    "        self.self.files = [p for p in glob.glob(os.path.join(root, \"**\", \"*\"), recursive=True)\n",
    "              if p.lower().endswith(exts)]\n",
    "        self.frames = frames\n",
    "        self._index = []\n",
    "        for p in self.files:\n",
    "            try:\n",
    "                info = sf.info(p)\n",
    "                if info.samplerate <= 0 or info.frames < SR * 2:\n",
    "                    continue\n",
    "                self._index.append(p)\n",
    "            except Exception:\n",
    "                continue\n",
    "        assert len(self._index) > 0, f\"No valid audio found under {root}\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._index)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self._index[idx]\n",
    "        y, sr = librosa.load(path, sr=SR, mono=True)\n",
    "        if len(y) < (self.frames+2)*HOP:\n",
    "            # pad if short\n",
    "            pad = (self.frames+2)*HOP - len(y)\n",
    "            y = np.pad(y, (0, pad))\n",
    "        # random crop (around frames)\n",
    "        max_start = max(0, len(y) - (self.frames+2)*HOP)\n",
    "        start = 0 if max_start == 0 else np.random.randint(0, max_start+1)\n",
    "        y = y[start:start + (self.frames*HOP + WIN)]\n",
    "        logmel = wav_to_logmel(y)\n",
    "        # ensure exact frame length\n",
    "        if logmel.shape[1] < self.frames:\n",
    "            pad_w = self.frames - logmel.shape[1]\n",
    "            logmel = np.pad(logmel, ((0,0),(0,pad_w)), mode='edge')\n",
    "        logmel = logmel[:, :self.frames]\n",
    "        x = torch.from_numpy(logmel).float().unsqueeze(0)  # [1, n_mels, frames]\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "05b7f9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Models: simple conv discriminator & generator on mel space\n",
    "# ----------------------------\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        ch = [1, 32, 64, 128, 256]\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(ch[0], ch[1], 4, 2, 1), nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(ch[1], ch[2], 4, 2, 1), nn.InstanceNorm2d(ch[2], affine=True), nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(ch[2], ch[3], 4, 2, 1), nn.InstanceNorm2d(ch[3], affine=True), nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(ch[3], ch[4], 4, 2, 1), nn.InstanceNorm2d(ch[4], affine=True), nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "        # project to scalar\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Conv2d(ch[4], 1, kernel_size=4, stride=1, padding=0),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.net(x)\n",
    "        # adaptive pooling to handle variable dims (if frames change)\n",
    "        h = F.adaptive_avg_pool2d(h, (1,1))\n",
    "        score = self.out(h).view(x.size(0))\n",
    "        return score\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim=128, out_h=N_MELS, out_w=FRAMES):\n",
    "        super().__init__()\n",
    "        self.out_h = out_h\n",
    "        self.out_w = out_w\n",
    "        base = 256\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(z_dim, base*8), nn.ReLU(True),\n",
    "            nn.Linear(base*8, base*16), nn.ReLU(True),\n",
    "        )\n",
    "        # reshape to a small 2D map and upsample with convTranspose\n",
    "        self.start_h, self.start_w = out_h//16, out_w//16\n",
    "        self.proj = nn.Linear(base*16, 256 * self.start_h * self.start_w)\n",
    "        self.up = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1), nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1), nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(64, 32, 4, 2, 1), nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(32, 16, 4, 2, 1), nn.ReLU(True),\n",
    "            nn.Conv2d(16, 1, 3, 1, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        h = self.fc(z)\n",
    "        h = self.proj(h)\n",
    "        h = h.view(z.size(0), 256, self.start_h, self.start_w)\n",
    "        x = self.up(h)\n",
    "        # output unconstrained log-mel; optionally clamp for stability\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e5d09a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Training windows: ~2.97s | Mel: 128x256\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# WGAN-GP training (REAL only)\n",
    "# ----------------------------\n",
    "\n",
    "def gradient_penalty(D, real, fake):\n",
    "    bsz = real.size(0)\n",
    "    eps = torch.rand(bsz, 1, 1, 1, device=real.device)\n",
    "    inter = eps * real + (1 - eps) * fake\n",
    "    inter.requires_grad_(True)\n",
    "    d_inter = D(inter)\n",
    "    grads = torch.autograd.grad(\n",
    "        outputs=d_inter, inputs=inter,\n",
    "        grad_outputs=torch.ones_like(d_inter),\n",
    "        create_graph=True, retain_graph=True, only_inputs=True\n",
    "    )[0]\n",
    "    grads = grads.view(bsz, -1)\n",
    "    gp = ((grads.norm(2, dim=1) - 1.0) ** 2).mean()\n",
    "    return gp\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample_mels(G, n=8, z_dim=128):\n",
    "    G.eval()\n",
    "    z = torch.randn(n, z_dim, device=DEVICE)\n",
    "    mels = G(z).cpu().numpy()  # [n,1,N_MELS,FRAMES]\n",
    "    return mels\n",
    "\n",
    "@torch.no_grad()\n",
    "def save_audio_samples(G, step_tag: str, n=4, z_dim=128):\n",
    "    mels = sample_mels(G, n=n, z_dim=z_dim)\n",
    "    for i, mel in enumerate(mels):\n",
    "        logmel = mel[0]\n",
    "        y = logmel_to_wav(logmel)\n",
    "        y = np.clip(y, -1.0, 1.0)\n",
    "        sf.write(f\"{SAMPLES_DIR}/sample_{step_tag}_{i:02d}.wav\", y, SR)\n",
    "\n",
    "\n",
    "def train_real_only_wgan_gp():\n",
    "    torch.manual_seed(SEED); np.random.seed(SEED); random.seed(SEED)\n",
    "    ds = RealMelDataset(REAL_AUDIO_DIR, frames=FRAMES)\n",
    "    dl = DataLoader(ds, batch_size=BATCH, shuffle=True, num_workers=2, drop_last=True)\n",
    "\n",
    "    D = Discriminator().to(DEVICE)\n",
    "    G = Generator(z_dim=128, out_h=N_MELS, out_w=FRAMES).to(DEVICE)\n",
    "    optD = torch.optim.Adam(D.parameters(), lr=LR_D, betas=BETAS)\n",
    "    optG = torch.optim.Adam(G.parameters(), lr=LR_G, betas=BETAS)\n",
    "\n",
    "    global_step = 0\n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "        pbar = tqdm(dl, desc=f\"Epoch {epoch}/{EPOCHS}\")\n",
    "        for real in pbar:\n",
    "            real = real.to(DEVICE)\n",
    "            bsz = real.size(0)\n",
    "            # --------------------\n",
    "            # (1) Update D\n",
    "            # --------------------\n",
    "            for _ in range(N_CRITIC):\n",
    "                z = torch.randn(bsz, 128, device=DEVICE)\n",
    "                fake = G(z).detach()\n",
    "                d_real = D(real)\n",
    "                d_fake = D(fake)\n",
    "                gp = gradient_penalty(D, real, fake)\n",
    "                lossD = (d_fake - d_real).mean() + LAMBDA_GP * gp\n",
    "                optD.zero_grad(set_to_none=True)\n",
    "                lossD.backward()\n",
    "                optD.step()\n",
    "\n",
    "            # --------------------\n",
    "            # (2) Update G\n",
    "            # --------------------\n",
    "            z = torch.randn(bsz, 128, device=DEVICE)\n",
    "            fake = G(z)\n",
    "            lossG = -D(fake).mean()\n",
    "            optG.zero_grad(set_to_none=True)\n",
    "            lossG.backward()\n",
    "            optG.step()\n",
    "\n",
    "            global_step += 1\n",
    "            if global_step % 100 == 0:\n",
    "                pbar.set_postfix({\"lossD\": float(lossD.item()), \"lossG\": float(lossG.item())})\n",
    "            if global_step % 1000 == 0:\n",
    "                save_audio_samples(G, step_tag=f\"e{epoch}_s{global_step}\", n=4)\n",
    "        # end epoch\n",
    "        torch.save(G.state_dict(), f\"{CKPT_DIR}/G_epoch{epoch:03d}.pt\")\n",
    "        torch.save(D.state_dict(), f\"{CKPT_DIR}/D_epoch{epoch:03d}.pt\")\n",
    "        save_audio_samples(G, step_tag=f\"e{epoch}\", n=8)\n",
    "    return G, D\n",
    "\n",
    "# %% [markdown]\n",
    "# ---- Quickstart ----\n",
    "# 1) Put your real audio under REAL_AUDIO_DIR (any extension supported by soundfile/librosa)\n",
    "# 2) Run: G, D = train_real_only_wgan_gp()\n",
    "# 3) Generate new music snippets at any time: save_audio_samples(G, step_tag=\"manual\", n=8)\n",
    "#    Files will appear under SAMPLES_DIR.\n",
    "\n",
    "# %%\n",
    "if __name__ == \"__main__\":\n",
    "    print(f\"Device: {DEVICE}\")\n",
    "    print(f\"Training windows: ~{WINDOW_SEC:.2f}s | Mel: {N_MELS}x{FRAMES}\")\n",
    "    # Uncomment to train from script:\n",
    "    # G, D = train_real_only_wgan_gp()\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1ad32783",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'RealMelDataset' object has no attribute 'self'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m G, D \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_real_only_wgan_gp\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m save_audio_samples(G, step_tag\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmanual\u001b[39m\u001b[38;5;124m\"\u001b[39m, n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m)\n",
      "Cell \u001b[1;32mIn[38], line 39\u001b[0m, in \u001b[0;36mtrain_real_only_wgan_gp\u001b[1;34m()\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtrain_real_only_wgan_gp\u001b[39m():\n\u001b[0;32m     38\u001b[0m     torch\u001b[38;5;241m.\u001b[39mmanual_seed(SEED); np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(SEED); random\u001b[38;5;241m.\u001b[39mseed(SEED)\n\u001b[1;32m---> 39\u001b[0m     ds \u001b[38;5;241m=\u001b[39m \u001b[43mRealMelDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mREAL_AUDIO_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFRAMES\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m     dl \u001b[38;5;241m=\u001b[39m DataLoader(ds, batch_size\u001b[38;5;241m=\u001b[39mBATCH, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, drop_last\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     42\u001b[0m     D \u001b[38;5;241m=\u001b[39m Discriminator()\u001b[38;5;241m.\u001b[39mto(DEVICE)\n",
      "Cell \u001b[1;32mIn[36], line 4\u001b[0m, in \u001b[0;36mRealMelDataset.__init__\u001b[1;34m(self, root, frames)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, root: \u001b[38;5;28mstr\u001b[39m, frames: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m FRAMES):\n\u001b[0;32m      3\u001b[0m     exts \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.wav\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.flac\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.mp3\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.ogg\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.m4a\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.aiff\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.aif\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[38;5;241m.\u001b[39mfiles \u001b[38;5;241m=\u001b[39m [p \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m glob\u001b[38;5;241m.\u001b[39mglob(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(root, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m**\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m), recursive\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      5\u001b[0m           \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;241m.\u001b[39mendswith(exts)]\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframes \u001b[38;5;241m=\u001b[39m frames\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'RealMelDataset' object has no attribute 'self'"
     ]
    }
   ],
   "source": [
    "G, D = train_real_only_wgan_gp()\n",
    "save_audio_samples(G, step_tag=\"manual\", n=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f32df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def spectral_conv2d(in_ch, out_ch, k, s, p):\n",
    "#     return nn.utils.spectral_norm(nn.Conv2d(in_ch, out_ch, k, s, p))\n",
    "\n",
    "# class Discriminator(nn.Module):\n",
    "#     \"\"\"PatchGAN-style: accepts [B,1,128,256] and outputs [B,1] scalar scores via global pooling.\"\"\"\n",
    "#     def __init__(self, use_spectral_norm=True):\n",
    "#         super().__init__()\n",
    "#         Conv = spectral_conv2d if use_spectral_norm else nn.Conv2d\n",
    "#         chs = [1, 32, 64, 128, 256, 256]\n",
    "#         self.net = nn.Sequential(\n",
    "#             Conv(chs[0], chs[1],  (3,3), (2,2), (1,1)), nn.LeakyReLU(0.2, inplace=True),\n",
    "#             Conv(chs[1], chs[2],  (3,3), (2,2), (1,1)), nn.LeakyReLU(0.2, inplace=True),\n",
    "#             Conv(chs[2], chs[3],  (3,3), (2,2), (1,1)), nn.LeakyReLU(0.2, inplace=True),\n",
    "#             Conv(chs[3], chs[4],  (3,3), (2,2), (1,1)), nn.LeakyReLU(0.2, inplace=True),\n",
    "#             Conv(chs[4], chs[5],  (3,3), (2,2), (1,1)), nn.LeakyReLU(0.2, inplace=True),\n",
    "#         )\n",
    "#         self.head = nn.Linear(chs[5], 1)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # x: [B,1,128,256]\n",
    "#         feat = self.net(x)                     # [B,C,h,w]\n",
    "#         feat = feat.mean(dim=(2,3))            # global average pool -> [B,C]\n",
    "#         out = self.head(feat)                  # [B,1]\n",
    "#         return out\n",
    "\n",
    "# class MelEncoder(nn.Module):\n",
    "#     \"\"\"Encode [B,1,128,256] -> [B,256,8,16] feature map (to match G's bottleneck).\"\"\"\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         chs = [1, 32, 64, 128, 256]\n",
    "#         self.net = nn.Sequential(\n",
    "#             nn.Conv2d(chs[0], chs[1], 3, 2, 1), nn.LeakyReLU(0.2, inplace=True),\n",
    "#             nn.Conv2d(chs[1], chs[2], 3, 2, 1), nn.LeakyReLU(0.2, inplace=True),\n",
    "#             nn.Conv2d(chs[2], chs[3], 3, 2, 1), nn.LeakyReLU(0.2, inplace=True),\n",
    "#             nn.Conv2d(chs[3], chs[4], 3, 2, 1), nn.LeakyReLU(0.2, inplace=True),\n",
    "#         )\n",
    "#     def forward(self, x):  # x: [B,1,128,256]\n",
    "#         return self.net(x)  # [B,256,8,16]\n",
    "\n",
    "# class CondGenerator(nn.Module):\n",
    "#     \"\"\"\n",
    "#     Conditional G: takes encoder(feature_of_real) + noise; outputs [B,1,128,256] mel.\n",
    "#     \"\"\"\n",
    "#     def __init__(self, z_dim=128):\n",
    "#         super().__init__()\n",
    "#         self.z_dim = z_dim\n",
    "#         self.enc_proj = nn.Conv2d(256, 256, 1)\n",
    "#         self.z_fc = nn.Linear(z_dim, 256*8*16)\n",
    "\n",
    "#         def block(in_ch, out_ch):\n",
    "#             return nn.Sequential(\n",
    "#                 nn.ConvTranspose2d(in_ch, out_ch, 4, 2, 1),\n",
    "#                 nn.BatchNorm2d(out_ch),\n",
    "#                 nn.ReLU(inplace=True)\n",
    "#             )\n",
    "\n",
    "#         self.up = nn.Sequential(\n",
    "#             block(512, 256),   # (enc 256 + noise 256) -> 256\n",
    "#             block(256, 128),\n",
    "#             block(128, 64),\n",
    "#             block(64, 32),\n",
    "#             nn.ConvTranspose2d(32, 1, 3, 1, 1),\n",
    "#             nn.Tanh()\n",
    "#         )\n",
    "\n",
    "#     def forward(self, enc_feat, z):\n",
    "#         # enc_feat: [B,256,8,16], z: [B, z_dim]\n",
    "#         enc = self.enc_proj(enc_feat)\n",
    "#         z = self.z_fc(z).view(z.size(0), 256, 8, 16)\n",
    "#         x = torch.cat([enc, z], dim=1)  # [B,512,8,16]\n",
    "#         x = self.up(x)                  # [B,1,~128,~256]\n",
    "#         x = F.interpolate(x, size=(N_MELS, FRAMES), mode=\"bilinear\", align_corners=False)\n",
    "#         return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e70922",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arthu\\Documents\\GAN_Music_Generator\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# def grad_penalty(D, real, fake, device):\n",
    "#     B = real.size(0)\n",
    "#     eps = torch.rand(B, 1, 1, 1, device=device)\n",
    "#     x_hat = eps * real + (1 - eps) * fake\n",
    "#     x_hat.requires_grad_(True)\n",
    "#     d_hat = D(x_hat)\n",
    "#     ones = torch.ones_like(d_hat, device=device)\n",
    "#     grads = torch.autograd.grad(\n",
    "#         outputs=d_hat, inputs=x_hat, grad_outputs=ones,\n",
    "#         create_graph=True, retain_graph=True, only_inputs=True\n",
    "#     )[0]\n",
    "#     gp = ((grads.view(B, -1).norm(2, dim=1) - 1.0) ** 2).mean()\n",
    "#     return gp\n",
    "\n",
    "# from tqdm.auto import tqdm\n",
    "\n",
    "# def train_wgan_gp(train_loader, epochs=EPOCHS, z_dim=Z_DIM, device=DEVICE):\n",
    "#     if train_loader is None:\n",
    "#         print(\"No training data. Populate REAL_WAV_DIR with .wav files and rerun.\")\n",
    "#         return None, None\n",
    "\n",
    "#     E = MelEncoder().to(device)\n",
    "#     G = CondGenerator(z_dim=Z_DIM).to(device)\n",
    "#     D = Discriminator().to(device)\n",
    "\n",
    "#     optD = torch.optim.Adam(D.parameters(), lr=LR_D, betas=(BETA1, BETA2))\n",
    "#     optG = torch.optim.Adam(G.parameters(), lr=LR_G, betas=(BETA1, BETA2))\n",
    "\n",
    "#     global_step = 0\n",
    "#     for epoch in range(1, epochs + 1):\n",
    "#         pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{epochs}\", leave=True)\n",
    "#         for mel in pbar:\n",
    "#             mel = mel.to(device)\n",
    "#             B = mel.size(0)\n",
    "#             z = torch.randn(B, Z_DIM, device=device)\n",
    "\n",
    "#             with torch.no_grad():\n",
    "#                 enc = E(mel)\n",
    "\n",
    "#             fake = G(enc, z)\n",
    "\n",
    "#             # ===== Train D (N_CRITIC steps) =====\n",
    "#             loss_D_val = 0.0\n",
    "#             for _ in range(N_CRITIC):\n",
    "#                 z = torch.randn(B, z_dim, device=device)\n",
    "#                 fake = G(z).detach()\n",
    "#                 d_real = D(mel).mean()\n",
    "#                 d_fake = D(fake.detach()).mean()\n",
    "#                 gp = grad_penalty(D, mel, fake.detach(), device)\n",
    "#                 loss_D = (d_fake - d_real) + LAMBDA_GP * gp\n",
    "\n",
    "#                 optD.zero_grad(set_to_none=True)\n",
    "#                 loss_D.backward()\n",
    "#                 optD.step()\n",
    "#                 loss_D_val = loss_D.item()\n",
    "\n",
    "#             # ===== Train G (1 step) =====\n",
    "#             z = torch.randn(B, z_dim, device=device)\n",
    "#             fake = G(z)\n",
    "#             loss_G = -D(fake).mean()\n",
    "#             optG.zero_grad(set_to_none=True)\n",
    "#             loss_G.backward()\n",
    "#             optG.step()\n",
    "#             d_fake_for_G = D(fake).mean()\n",
    "#             lambda_rec = 10.0  # tweak 1–20; higher = closer to input, lower = freer stylization\n",
    "#             rec_loss = F.l1_loss(fake, mel)\n",
    "#             loss_G = -d_fake_for_G + lambda_rec * rec_loss\n",
    "\n",
    "#             optG.zero_grad(set_to_none=True)\n",
    "#             loss_G.backward()\n",
    "#             optG.step()\n",
    "\n",
    "\n",
    "#             # tqdm stats\n",
    "#             pbar.set_postfix({\n",
    "#                 \"D\": f\"{loss_D_val:.3f}\",\n",
    "#                 \"G\": f\"{loss_G.item():.3f}\",\n",
    "#                 \"d_real\": f\"{d_real.item():.3f}\",\n",
    "#                 \"d_fake\": f\"{d_fake.item():.3f}\"\n",
    "#             })\n",
    "#             global_step += 1\n",
    "\n",
    "#     return G, D\n",
    "# # Quick dry-run (skips if no data)\n",
    "# # G, D = train_wgan_gp(train_loader, epochs=1)\n",
    "# # torch.save(D.state_dict(), \"D.pth\"); torch.save(G.state_dict(), \"G.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fadead",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# @torch.no_grad()\n",
    "# def score_windows_with_D(D, mels, device=DEVICE):\n",
    "#     D.eval()\n",
    "#     scores = []\n",
    "#     for i in range(0, len(mels), 16):\n",
    "#         batch = torch.stack(mels[i:i+16], dim=0).to(device)  # [B,1,128,256]\n",
    "#         s = D(batch).squeeze(1).detach().cpu().numpy()\n",
    "#         scores.extend(s.tolist())\n",
    "#     return float(np.mean(scores)), scores\n",
    "\n",
    "# def slice_track_to_mels(path, codec_parity_for_real=False, step_secs=10.0):\n",
    "#     \"\"\"Return list of [1,128,256] mel windows for an entire track.\n",
    "#     - For real WAVs, pass codec_parity_for_real=True to round-trip MP3.\n",
    "#     - For AI MP3s, leave False (already MP3).\n",
    "#     \"\"\"\n",
    "#     wav = load_audio(path, sr=SR)\n",
    "#     if codec_parity_for_real:\n",
    "#         wav = mp3_roundtrip(wav, sr=SR)\n",
    "#     wav = lufs_normalize(wav, sr=SR, target_lufs=TARGET_LUFS)\n",
    "\n",
    "#     N = int(SR * WIN_SECS)\n",
    "#     step = int(SR * step_secs)\n",
    "#     if wav.numel() < N:\n",
    "#         wav = F.pad(wav, (0, N - wav.numel()))\n",
    "\n",
    "#     mels = []\n",
    "#     for start in range(0, max(1, wav.numel() - N + 1), step):\n",
    "#         seg = wav[start:start+N]\n",
    "#         mel = to_logmel(seg)\n",
    "#         mel = fix_frames(mel, FRAMES)\n",
    "#         mels.append(mel)\n",
    "#     return mels  # list of [1,128,256]\n",
    "\n",
    "# def score_folder(D, folder, exts, real_folder=False, max_files=None):\n",
    "#     files = sorted([p for ext in exts for p in Path(folder).rglob(f\"*{ext}\")])\n",
    "#     if max_files is not None:\n",
    "#         files = files[:max_files]\n",
    "\n",
    "#     kept_files, kept_scores = [], []\n",
    "#     for p in files:\n",
    "#         try:\n",
    "#             codec_parity = real_folder  # True for real WAVs, False for AI MP3s\n",
    "#             mels = slice_track_to_mels(p, codec_parity_for_real=codec_parity, step_secs=WIN_SECS)\n",
    "#             if not mels:\n",
    "#                 continue\n",
    "#             mean_s, _ = score_windows_with_D(D, mels)\n",
    "#             kept_files.append(p)\n",
    "#             kept_scores.append(mean_s)\n",
    "#         except Exception as e:\n",
    "#             # Skip problematic file; keep evaluation running\n",
    "#             print(f\"[skip] {p} -> {e}\")\n",
    "#             continue\n",
    "\n",
    "#     return kept_files, np.array(kept_scores, dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a0f197",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def evaluate_discriminator(D, real_dir=REAL_WAV_DIR, ai_dir=AI_MP3_DIR, max_files=MAX_EVAL_FILES_PER_CLASS):\n",
    "#     if not HAVE_SK:\n",
    "#         print(\"scikit-learn not found — skipping ROC-AUC/PR-AUC. You can 'pip install scikit-learn' and rerun.\")\n",
    "#         return None\n",
    "\n",
    "#     real_files, real_scores = score_folder(D, real_dir, [\".wav\", \".WAV\"], real_folder=True, max_files=max_files)\n",
    "#     ai_files,   ai_scores   = score_folder(D, ai_dir,   [\".mp3\", \".MP3\"], real_folder=False, max_files=max_files)\n",
    "\n",
    "#     y_true = np.array([1]*len(real_scores) + [0]*len(ai_scores), dtype=np.int32)\n",
    "#     y_pred = np.concatenate([real_scores, ai_scores], axis=0)\n",
    "\n",
    "#     roc = roc_auc_score(y_true, y_pred)\n",
    "#     pr  = average_precision_score(y_true, y_pred)\n",
    "#     print(f\"ROC-AUC: {roc:.4f} | PR-AUC: {pr:.4f}\")\n",
    "#     return {\n",
    "#         \"roc_auc\": float(roc),\n",
    "#         \"pr_auc\": float(pr),\n",
    "#         \"real_scores\": real_scores,\n",
    "#         \"ai_scores\": ai_scores,\n",
    "#         \"real_files\": [str(p) for p in real_files],\n",
    "#         \"ai_files\": [str(p) for p in ai_files],\n",
    "#     }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20bffa2",
   "metadata": {},
   "source": [
    "\n",
    "## Quickstart\n",
    "\n",
    "1. Put your data here (or change the config):\n",
    "   - `data/real_wav/**.wav`\n",
    "   - `data/ai_mp3/**.mp3`\n",
    "\n",
    "2. Run training:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71e79ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:   0%|          | 0/124 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "CondGenerator.forward() missing 1 required positional argument: 'z'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Train the GAN on REAL only (WGAN-GP). Increase EPOCHS later.\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m G, D \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_wgan_gp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Save checkpoints\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m D \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m G \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "Cell \u001b[1;32mIn[9], line 46\u001b[0m, in \u001b[0;36mtrain_wgan_gp\u001b[1;34m(train_loader, epochs, z_dim, device)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(N_CRITIC):\n\u001b[0;32m     45\u001b[0m     z \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(B, z_dim, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m---> 46\u001b[0m     fake \u001b[38;5;241m=\u001b[39m \u001b[43mG\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[0;32m     47\u001b[0m     d_real \u001b[38;5;241m=\u001b[39m D(mel)\u001b[38;5;241m.\u001b[39mmean()\n\u001b[0;32m     48\u001b[0m     d_fake \u001b[38;5;241m=\u001b[39m D(fake\u001b[38;5;241m.\u001b[39mdetach())\u001b[38;5;241m.\u001b[39mmean()\n",
      "File \u001b[1;32mc:\\Users\\arthu\\Documents\\GAN_Music_Generator\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\arthu\\Documents\\GAN_Music_Generator\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[1;31mTypeError\u001b[0m: CondGenerator.forward() missing 1 required positional argument: 'z'"
     ]
    }
   ],
   "source": [
    "\n",
    "# # Train the GAN on REAL only (WGAN-GP). Increase EPOCHS later.\n",
    "# G, D = train_wgan_gp(train_loader, epochs=EPOCHS)\n",
    "\n",
    "# # Save checkpoints\n",
    "# if D is not None and G is not None:\n",
    "#     torch.save(D.state_dict(), \"D.pth\")\n",
    "#     torch.save(G.state_dict(), \"G.pth\")\n",
    "#     print(\"Saved D.pth and G.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2a356a",
   "metadata": {},
   "source": [
    "\n",
    "3. Score folders and compute metrics:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f649a32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arthu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\librosa\\core\\intervals.py:8: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_filename\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC: 0.1468 | PR-AUC: 0.3437\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# if 'D' in globals() and D is not None:\n",
    "#     _ = evaluate_discriminator(D, real_dir=REAL_WAV_DIR, ai_dir=AI_MP3_DIR, max_files=MAX_EVAL_FILES_PER_CLASS)\n",
    "# else:\n",
    "#     print(\"Train (or load) a Discriminator first.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e587738",
   "metadata": {},
   "source": [
    "\n",
    "### Load Discriminator later & score single files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d08469c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean realism score: -14.58976697921753\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# def load_discriminator(path=\"D.pth\", device=DEVICE):\n",
    "#     D = Discriminator(use_spectral_norm=True).to(device)\n",
    "#     sd = torch.load(path, map_location=device)\n",
    "#     D.load_state_dict(sd)\n",
    "#     D.eval()\n",
    "#     return D\n",
    "\n",
    "# # Example single-file scoring (edit the paths):\n",
    "# D = load_discriminator(\"D.pth\")\n",
    "# mels = slice_track_to_mels(\"data/AI_audio/-0Gj8-vB1q4_1.mp3\", codec_parity_for_real=False, step_secs=WIN_SECS)\n",
    "# mean_score, window_scores = score_windows_with_D(D, mels)\n",
    "# print(\"Mean realism score:\", mean_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9816f3d3",
   "metadata": {},
   "source": [
    "\n",
    "## Notes & Tips\n",
    "\n",
    "- **Codec parity matters**: we round-trip real WAVs through MP3 (192 kbps) during training and when *scoring* real files.  \n",
    "  If `ffmpeg` isn't available, the notebook will silently skip parity (you can install ffmpeg to enable it).\n",
    "\n",
    "- **Loudness normalization**: LUFS if `pyloudnorm` is installed, else peak-normalization fallback.\n",
    "\n",
    "- **Input shape fixed**: mel specs resized to `[1, 128, 256]`. You can increase `FRAMES` (e.g., 512) if your GPU has room.\n",
    "\n",
    "- **Stability**: We use **WGAN-GP** and **spectral norm** on the discriminator.\n",
    "\n",
    "- **Sanity check**: Before full training, try a few batches to ensure losses move and `d_real > d_fake` early on.\n",
    "\n",
    "- **Evaluation**: The **mean discriminator score** per track is your realism score. With scikit-learn installed, we report **ROC-AUC** and **PR-AUC**.\n",
    "\n",
    "- **Next steps**: try **PCEN** instead of dB, add light augmentations, or move to longer windows, then compare metrics.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
