{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b13b5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% ------------------------------------ imports\n",
    "import math\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import librosa\n",
    "\n",
    "# ==========================\n",
    "# Hyperparams (same semantics)\n",
    "# ==========================\n",
    "min_signal_rate = 0.02\n",
    "max_signal_rate = 0.95\n",
    "ema = 0.999\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89bf28b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% ------------------------------------ losses / metrics (PyTorch)\n",
    "\n",
    "def spectral_norm_diff(pred: torch.Tensor, real: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Difference in 'spectral norm' between batches, analogous to your TF helper.\n",
    "    NOTE: renamed to avoid clashing with torch.nn.utils.spectral_norm.\n",
    "    real/pred: (B, H, W, C) or (B, C, H, W) – we convert to (B, -1)\n",
    "    \"\"\"\n",
    "    if pred.dim() == 4 and pred.shape[1] not in (1, 2, 3):  # probably NHWC\n",
    "        # NHWC -> NCHW\n",
    "        pred = pred.permute(0, 3, 1, 2).contiguous()\n",
    "        real = real.permute(0, 3, 1, 2).contiguous()\n",
    "    # flatten spatial + channel\n",
    "    pr = pred.flatten(1)\n",
    "    rr = real.flatten(1)\n",
    "    norm_real = torch.norm(rr, dim=1) + 1e-6\n",
    "    norm_pred = torch.norm(pr, dim=1) + 1e-6\n",
    "    return torch.mean(torch.abs(norm_real - norm_pred) / norm_real)\n",
    "\n",
    "\n",
    "def time_derivative_loss(pred: torch.Tensor, real: torch.Tensor, window: int = 1) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Match finite differences along the 'time' dimension (assume H is time).\n",
    "    Works for NCHW or NHWC (we'll align to NCHW internally).\n",
    "    \"\"\"\n",
    "    if pred.dim() == 4 and pred.shape[1] not in (1, 2, 3):  # NHWC -> NCHW\n",
    "        pred = pred.permute(0, 3, 1, 2)\n",
    "        real = real.permute(0, 3, 1, 2)\n",
    "    # pred, real: (B, C, H, W)\n",
    "    real_dx = real[:, :, :-window, :] - real[:, :, window:, :]\n",
    "    pred_dx = pred[:, :, :-window, :] - pred[:, :, window:, :]\n",
    "    return F.mse_loss(pred_dx, real_dx)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd0a5eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_mdct_from_file(file: str, idx: int, rate: int = 10_000, feats: int = 256, duration: float = 3.3) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    MDCT via windowed DCT-IV with 50% overlap.\n",
    "    Returns array shape (feats, feats//2) to match your pipeline.\n",
    "    \"\"\"\n",
    "    # load audio robustly\n",
    "    audio, _ = _safe_audio_load(file, sr=rate, offset=idx, duration=duration)\n",
    "\n",
    "    # pad / truncate to exact length\n",
    "    target_len = int(rate * duration)\n",
    "    audio_fill = np.zeros(target_len, dtype=np.float32)\n",
    "    audio_fill[:min(len(audio), target_len)] = audio[:target_len]\n",
    "\n",
    "    # framing\n",
    "    N = feats            # MDCT bins (half window)\n",
    "    win_len = 2 * N      # frame length\n",
    "    hop = N              # 50% overlap\n",
    "\n",
    "    # frames: expected shape (win_len, n_frames)\n",
    "    frames = librosa.util.frame(audio_fill, frame_length=win_len, hop_length=hop)\n",
    "    if frames.shape[0] != win_len and frames.shape[1] == win_len:\n",
    "        frames = frames.swapaxes(0, 1)  # enforce (win_len, n_frames)\n",
    "\n",
    "    # sine window along axis=0\n",
    "    window = np.sin(np.pi / win_len * (np.arange(win_len) + 0.5)).astype(np.float32)  # (win_len,)\n",
    "    frames = frames * window[:, None]  # (win_len, n_frames)\n",
    "\n",
    "    # MDCT = DCT-IV along axis=0, keep first N coeffs\n",
    "    mdct = dct(frames, type=4, norm=\"ortho\", axis=0)[:N, :]  # (N, n_frames)\n",
    "\n",
    "    # crop/pad to (feats, feats//2)\n",
    "    H, W = feats, feats // 2\n",
    "    out = np.zeros((H, W), dtype=np.float32)\n",
    "    h = min(H, mdct.shape[0])\n",
    "    w = min(W, mdct.shape[1])\n",
    "    out[:h, :w] = mdct[:h, :w]\n",
    "    return out\n",
    "\n",
    "\n",
    "def _is_decodable(path: str) -> bool:\n",
    "    \"\"\"\n",
    "    Quick pre-check to skip obviously undecodable files.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import torchaudio\n",
    "        torchaudio.info(path)\n",
    "        return True\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        import soundfile as sf\n",
    "        with sf.SoundFile(path):\n",
    "            return True\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        from scipy.io import wavfile\n",
    "        wavfile.read(path)\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "# %% =============================== dataset / dataloader\n",
    "\n",
    "class FilesMDCTDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        glob_location: str,\n",
    "        total_seconds: int = 2,\n",
    "        out_len: float = 3.3,\n",
    "        hop_size: int = 1,\n",
    "        max_feats: int = 2048,\n",
    "        batch_scale: float = 1.0,\n",
    "        rate: int = 10_000,\n",
    "        mdct_feats: int = 256,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        all_files = glob(glob_location, recursive=True)\n",
    "        self.files = [f for f in all_files if _is_decodable(f)]\n",
    "        if len(self.files) == 0:\n",
    "            raise RuntimeError(\"No decodable audio files found. Check backends or path.\")\n",
    "        pairs = []\n",
    "        for s in range(total_seconds):\n",
    "            for f in self.files:\n",
    "                pairs.append((f, s * hop_size))\n",
    "        self.pairs = pairs\n",
    "        self.rate = rate\n",
    "        self.out_len = out_len\n",
    "        self.mdct_feats = mdct_feats\n",
    "        self.max_feats = max_feats\n",
    "        self.scale = batch_scale\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, i: int) -> torch.Tensor:\n",
    "        f, idx = self.pairs[i]\n",
    "        spec = _load_mdct_from_file(\n",
    "            f, idx, rate=self.rate, feats=self.mdct_feats, duration=self.out_len\n",
    "        )  # (H=feats, W=feats//2)\n",
    "\n",
    "        # ensure shape (max_feats, mdct_feats//2, 1)\n",
    "        H = self.max_feats\n",
    "        W = self.mdct_feats // 2\n",
    "        out = np.zeros((H, W), dtype=np.float32)\n",
    "        h = min(H, spec.shape[0])\n",
    "        w = min(W, spec.shape[1])\n",
    "        out[:h, :w] = spec[:h, :w]\n",
    "        out = out[..., None] * self.scale  # (H, W, 1)\n",
    "\n",
    "        # return as CHW for PyTorch (C, H, W)\n",
    "        out = np.transpose(out, (2, 0, 1))  # (1, H, W)\n",
    "        return torch.from_numpy(out)\n",
    "\n",
    "\n",
    "def get_files_dataloader(\n",
    "    glob_location: str,\n",
    "    total_seconds: int = 2,\n",
    "    out_len: float = 3.3,\n",
    "    hop_size: int = 1,\n",
    "    max_feats: int = 256,\n",
    "    batch_size: int = 16,\n",
    "    shuffle_size: int = 1000,  # not used; kept for parity\n",
    "    scale: float = 1.0,\n",
    "    rate: int = 10_000,\n",
    "    mdct_feats: int = 256,\n",
    "    num_workers: int = 0,\n",
    "    pin_memory: bool = False,\n",
    ") -> DataLoader:\n",
    "    ds = FilesMDCTDataset(\n",
    "        glob_location=glob_location,\n",
    "        total_seconds=total_seconds,\n",
    "        out_len=out_len,\n",
    "        hop_size=hop_size,\n",
    "        max_feats=max_feats,\n",
    "        batch_scale=scale,\n",
    "        rate=rate,\n",
    "        mdct_feats=mdct_feats,\n",
    "    )\n",
    "    loader = DataLoader(\n",
    "        ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,  # 0 is safer in notebooks\n",
    "        pin_memory=pin_memory,\n",
    "        drop_last=True,\n",
    "    )\n",
    "    return loader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c6ba596",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% ------------------------------------ normalization module (adaptable)\n",
    "\n",
    "class AdaptiveNormalizer(nn.Module):\n",
    "    \"\"\"\n",
    "    Store mean/std estimated from data (like Keras Normalization).\n",
    "    Call .adapt(dataloader, steps=...) once before training.\n",
    "    \"\"\"\n",
    "    def __init__(self, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.register_buffer(\"mean\", torch.zeros(1, 1, 1))\n",
    "        self.register_buffer(\"std\", torch.ones(1, 1, 1))\n",
    "        self.eps = eps\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def adapt(self, dataloader, max_batches: int = 512, device=\"cpu\", show_progress: bool = True):\n",
    "        from tqdm.auto import tqdm\n",
    "\n",
    "        n = 0\n",
    "        mean = 0.0\n",
    "        M2 = 0.0\n",
    "        seen = 0\n",
    "        iterator = iter(dataloader)\n",
    "        rng = range(max_batches)\n",
    "        if show_progress:\n",
    "            rng = tqdm(rng, desc=\"adapting normalizer\", dynamic_ncols=True, leave=False)\n",
    "\n",
    "        for _ in rng:\n",
    "            try:\n",
    "                x = next(iterator)\n",
    "            except StopIteration:\n",
    "                break\n",
    "            x = x.to(device)  # (B, 1, H, W)\n",
    "            b = x.numel()\n",
    "            seen += b\n",
    "            delta = x - mean\n",
    "            mean = mean + delta.sum() / seen\n",
    "            delta2 = x - mean\n",
    "            M2 = M2 + (delta * delta2).sum()\n",
    "\n",
    "        var = M2 / max(1, seen - 1)\n",
    "        std = torch.sqrt(var + self.eps)\n",
    "        self.mean = torch.tensor([[[mean.item()]]], device=device)\n",
    "        self.std = torch.tensor([[[std.item()]]], device=device)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: (B, C, H, W)\n",
    "        return (x - self.mean) / (self.std + self.eps)\n",
    "\n",
    "    def denormalize(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return x * (self.std + self.eps) + self.mean\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb6f419c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% ------------------------------------ UNet-like noise predictor (with FiLM time conditioning)\n",
    "\n",
    "class FiLMBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, t_dim, use_attn=False):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n",
    "        self.gn1 = nn.GroupNorm(8, out_ch)\n",
    "        self.to_gamma = nn.Linear(t_dim, out_ch)\n",
    "        self.to_beta = nn.Linear(t_dim, out_ch)\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n",
    "        self.gn2 = nn.GroupNorm(8, out_ch)\n",
    "        self.use_attn = use_attn\n",
    "        if use_attn:\n",
    "            self.q = nn.Conv2d(out_ch, out_ch, 1)\n",
    "            self.k = nn.Conv2d(out_ch, out_ch, 1)\n",
    "            self.v = nn.Conv2d(out_ch, out_ch, 1)\n",
    "\n",
    "    def forward(self, x, t_emb):\n",
    "        # conv1 + FiLM (shift + bias in two steps, mirroring your TF idea)\n",
    "        h = self.conv1(x)\n",
    "        h = self.gn1(h)\n",
    "        gamma = self.to_gamma(t_emb).unsqueeze(-1).unsqueeze(-1)  # (B, C, 1, 1)\n",
    "        beta = self.to_beta(t_emb).unsqueeze(-1).unsqueeze(-1)\n",
    "        h = h + gamma\n",
    "        h = F.silu(h)\n",
    "        h = h + beta\n",
    "\n",
    "        h = self.conv2(h)\n",
    "        h = self.gn2(h)\n",
    "        h = F.silu(h)\n",
    "\n",
    "        if self.use_attn:\n",
    "            B, C, H, W = h.shape\n",
    "            q = self.q(h).flatten(2).transpose(1, 2)  # (B, HW, C)\n",
    "            k = self.k(h).flatten(2)                  # (B, C, HW)\n",
    "            v = self.v(h).flatten(2).transpose(1, 2)  # (B, HW, C)\n",
    "            attn = torch.softmax(q @ k / math.sqrt(C), dim=-1)  # (B, HW, HW)\n",
    "            h_attn = attn @ v                              # (B, HW, C)\n",
    "            h_attn = h_attn.transpose(1, 2).reshape(B, C, H, W)\n",
    "            h = h + h_attn\n",
    "        return h\n",
    "\n",
    "\n",
    "class TimeEmbed(nn.Module):\n",
    "    def __init__(self, dim=128):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(1, dim)\n",
    "        self.fc2 = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, t):  # t: (B, 1, 1, 1) or (B,1)\n",
    "        if t.dim() == 4:\n",
    "            t = t.view(t.size(0), 1)\n",
    "        x = F.silu(self.fc1(t))\n",
    "        x = F.silu(self.fc2(x))\n",
    "        return x  # (B, dim)\n",
    "\n",
    "\n",
    "class UNetNoisePredictor(nn.Module):\n",
    "    def __init__(self, widths, block_depth, attention=False, dim1=256, dim2=128, t_dim=128):\n",
    "        super().__init__()\n",
    "        self.dim1, self.dim2 = dim1, dim2\n",
    "        self.t_embed = TimeEmbed(t_dim)\n",
    "\n",
    "        chs = widths\n",
    "        downs = nn.ModuleList()\n",
    "        ups = nn.ModuleList()\n",
    "        self.skips_out = []\n",
    "\n",
    "        # Down\n",
    "        in_ch = 1\n",
    "        self.down_blocks = nn.ModuleList()\n",
    "        for i, ch in enumerate(chs):\n",
    "            blocks = nn.ModuleList()\n",
    "            for _ in range(block_depth):\n",
    "                blocks.append(FiLMBlock(in_ch, ch, t_dim, use_attn=attention and (i >= len(chs)//2)))\n",
    "                in_ch = ch\n",
    "            self.down_blocks.append(blocks)\n",
    "            if i < len(chs) - 1:\n",
    "                downs.append(nn.Conv2d(ch, ch, 3, stride=2, padding=1))\n",
    "        self.downs = downs\n",
    "\n",
    "        # Mid\n",
    "        self.mid = FiLMBlock(chs[-1], chs[-1], t_dim, use_attn=attention)\n",
    "\n",
    "        # Up\n",
    "        self.up_convs = nn.ModuleList()\n",
    "        self.up_blocks = nn.ModuleList()\n",
    "        for i, ch in reversed(list(enumerate(chs[:-1]))):\n",
    "            self.up_convs.append(nn.Sequential(\n",
    "                nn.Upsample(scale_factor=2, mode=\"nearest\"),\n",
    "                nn.Conv2d(chs[i+1], ch, 3, padding=1),\n",
    "            ))\n",
    "            blocks = nn.ModuleList()\n",
    "            for _ in range(block_depth):\n",
    "                blocks.append(FiLMBlock(ch*2, ch, t_dim, use_attn=attention and (i >= len(chs)//2)))\n",
    "            self.up_blocks.append(blocks)\n",
    "\n",
    "        self.out = nn.Conv2d(chs[0], 1, 1)\n",
    "\n",
    "    def forward(self, x, t_in):\n",
    "        \"\"\"\n",
    "        x: (B,1,H,W); t_in: (B,1,1,1) or (B,1) with values in [0,1] (we pass noise_rate**2)\n",
    "        \"\"\"\n",
    "        t_emb = self.t_embed(t_in)\n",
    "\n",
    "        # Down\n",
    "        skips = []\n",
    "        h = x\n",
    "        for i, blocks in enumerate(self.down_blocks):\n",
    "            for blk in blocks:\n",
    "                h = blk(h, t_emb)\n",
    "            skips.append(h)\n",
    "            if i < len(self.downs):\n",
    "                h = self.downs[i](h)\n",
    "\n",
    "        # Mid\n",
    "        h = self.mid(h, t_emb)\n",
    "\n",
    "        # Up\n",
    "        for up, blocks, skip in zip(self.up_convs, self.up_blocks, reversed(skips[:-1])):\n",
    "            h = up(h)\n",
    "            h = torch.cat([h, skip], dim=1)\n",
    "            for blk in blocks:\n",
    "                h = blk(h, t_emb)\n",
    "\n",
    "        return self.out(h)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acc6ee0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% ------------------------------------ DDIM model (PyTorch)\n",
    "\n",
    "class DDIMTorch(nn.Module):\n",
    "    def __init__(self, widths, block_depth, attention=False, dim1=256, dim2=128, device=\"cuda\"):\n",
    "        super().__init__()\n",
    "        self.device = torch.device(device if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.normalizer = AdaptiveNormalizer()\n",
    "        self.network = UNetNoisePredictor(widths, block_depth, attention=attention, dim1=dim1, dim2=dim2).to(self.device)\n",
    "        self.ema_network = UNetNoisePredictor(widths, block_depth, attention=attention, dim1=dim1, dim2=dim2).to(self.device)\n",
    "        self.ema_network.load_state_dict(self.network.state_dict())\n",
    "        self.spec_mod = 0.0\n",
    "        self.dx_mod = 0.0\n",
    "\n",
    "        self.mse = nn.MSELoss()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def diffusion_schedule(self, diffusion_times):\n",
    "        # diffusion_times: (B,1,1,1) in [0,1]\n",
    "        start_angle = math.acos(max_signal_rate)\n",
    "        end_angle = math.acos(min_signal_rate)\n",
    "        diffusion_angles = start_angle + diffusion_times * (end_angle - start_angle)\n",
    "        signal_rates = torch.cos(diffusion_angles)\n",
    "        noise_rates = torch.sin(diffusion_angles)\n",
    "        return noise_rates, signal_rates\n",
    "\n",
    "    def denoise(self, noisy_data, noise_rates, signal_rates, training=True):\n",
    "        net = self.network if training else self.ema_network\n",
    "        # pass noise_rates**2 like your TF code\n",
    "        cond = (noise_rates ** 2).to(noisy_data.dtype)\n",
    "        pred_noises = net(noisy_data, cond)\n",
    "        pred_data = (noisy_data - noise_rates * pred_noises) / (signal_rates + 1e-6)\n",
    "        return pred_noises, pred_data\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def reverse_diffusion(self, initial_noise, diffusion_steps: int):\n",
    "        B = initial_noise.size(0)\n",
    "        step_size = 1.0 / diffusion_steps\n",
    "        next_noisy = initial_noise\n",
    "        for step in tqdm(range(diffusion_steps)):\n",
    "            noisy = next_noisy\n",
    "            t = torch.ones((B, 1, 1, 1), device=self.device) - step * step_size\n",
    "            noise_rates, signal_rates = self.diffusion_schedule(t)\n",
    "            pred_noises, pred_data = self.denoise(noisy, noise_rates, signal_rates, training=False)\n",
    "            t_next = t - step_size\n",
    "            next_noise_rates, next_signal_rates = self.diffusion_schedule(t_next)\n",
    "            next_noisy = next_signal_rates * pred_data + next_noise_rates * pred_noises\n",
    "        return pred_data\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, num_examples, shape, diffusion_steps):\n",
    "        # shape: (C,H,W) – like your dataset (1, H, W)\n",
    "        initial_noise = torch.randn((num_examples, *shape), device=self.device)\n",
    "        generated = self.reverse_diffusion(initial_noise, diffusion_steps)\n",
    "        denorm = self.normalizer.denormalize(generated)\n",
    "        return torch.clamp(denorm, -128.0, 128.0)\n",
    "\n",
    "    def _get_losses(self, y_true, y_pred):\n",
    "        l = self.mse(y_pred, y_true)\n",
    "        s = spectral_norm_diff(y_pred, y_true)\n",
    "        d = time_derivative_loss(y_pred, y_true)\n",
    "        return l, s, d\n",
    "\n",
    "    def training_step(self, batch, optimizer):\n",
    "        batch = batch.to(self.device)  # (B,1,H,W)\n",
    "\n",
    "        data = self.normalizer(batch)\n",
    "        noises = torch.randn_like(data)\n",
    "\n",
    "        B = data.size(0)\n",
    "        diffusion_times = torch.rand((B, 1, 1, 1), device=self.device)\n",
    "        noise_rates, signal_rates = self.diffusion_schedule(diffusion_times)\n",
    "        noisy = signal_rates * data + noise_rates * noises\n",
    "\n",
    "        pred_noises, pred_data = self.denoise(noisy, noise_rates, signal_rates, training=True)\n",
    "        noise_loss, noise_spec, noise_dx = self._get_losses(noises, pred_noises)\n",
    "        data_loss, data_spec, data_dx = self._get_losses(data, pred_data)\n",
    "\n",
    "        total_noise_loss = noise_loss + self.spec_mod * noise_spec + self.dx_mod * noise_dx\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        total_noise_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # EMA\n",
    "        with torch.no_grad():\n",
    "            for p, q in zip(self.network.parameters(), self.ema_network.parameters()):\n",
    "                q.mul_(ema).add_(p, alpha=1.0 - ema)\n",
    "\n",
    "        metrics = {\n",
    "            \"n_loss\": noise_loss.detach().item(),\n",
    "            \"d_loss\": data_loss.detach().item(),\n",
    "            \"n_spec\": noise_spec.detach().item(),\n",
    "            \"d_spec\": data_spec.detach().item(),\n",
    "            \"n_dx\": noise_dx.detach().item(),\n",
    "            \"d_dx\": data_dx.detach().item(),\n",
    "            \"n_total\": total_noise_loss.detach().item(),\n",
    "            \"d_total\": (data_loss + self.spec_mod * data_spec + self.dx_mod * data_dx).detach().item(),\n",
    "        }\n",
    "        return metrics\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def eval_step(self, batch):\n",
    "        batch = batch.to(self.device)\n",
    "        data = self.normalizer(batch)\n",
    "        noises = torch.randn_like(data)\n",
    "\n",
    "        B = data.size(0)\n",
    "        diffusion_times = torch.rand((B, 1, 1, 1), device=self.device)\n",
    "        noise_rates, signal_rates = self.diffusion_schedule(diffusion_times)\n",
    "        noisy = signal_rates * data + noise_rates * noises\n",
    "\n",
    "        pred_noises, pred_data = self.denoise(noisy, noise_rates, signal_rates, training=False)\n",
    "        noise_loss = self.mse(noises, pred_noises)\n",
    "        data_loss = self.mse(data, pred_data)\n",
    "        return {\"n_loss\": noise_loss.item(), \"d_loss\": data_loss.item()}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9720f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% ------------------------------------ build dataloader (parity with your TF dataset)\n",
    "\n",
    "def get_files_dataloader(\n",
    "    glob_location,\n",
    "    total_seconds=2,\n",
    "    out_len=3.3,\n",
    "    hop_size=1,\n",
    "    max_feats=256,\n",
    "    batch_size=16,\n",
    "    shuffle_size=1000,  # not used exactly the same as TF shuffle; DataLoader shuffle=True is typical\n",
    "    scale=1.0,\n",
    "    rate=10_000,\n",
    "    mdct_feats=256,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "):\n",
    "    ds = FilesMDCTDataset(\n",
    "        glob_location=glob_location,\n",
    "        total_seconds=total_seconds,\n",
    "        out_len=out_len,\n",
    "        hop_size=hop_size,\n",
    "        max_feats=max_feats,\n",
    "        batch_scale=scale,\n",
    "        rate=rate,\n",
    "        mdct_feats=mdct_feats,\n",
    "    )\n",
    "    loader = DataLoader(\n",
    "        ds, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=pin_memory, drop_last=True\n",
    "    )\n",
    "    return loader\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d59fedb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arthu\\Documents\\GAN_Music_Generator\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name '_safe_audio_load' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 20\u001b[0m\n\u001b[0;32m      6\u001b[0m loader \u001b[38;5;241m=\u001b[39m get_files_dataloader(\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/REAL_audio/*.wav\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      8\u001b[0m     out_len\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3.3\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     16\u001b[0m     pin_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     17\u001b[0m )\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# peek a batch to get shape\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;66;03m# batch: (B, 1, H, W)\u001b[39;00m\n\u001b[0;32m     22\u001b[0m     shape \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mshape  \u001b[38;5;66;03m# torch.Size([B, 1, H, W])\u001b[39;00m\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\arthu\\Documents\\GAN_Music_Generator\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:732\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    729\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    730\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    731\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 732\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    733\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    734\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    735\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    736\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    737\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    738\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\arthu\\Documents\\GAN_Music_Generator\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:788\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    786\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    787\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 788\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    789\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    790\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\arthu\\Documents\\GAN_Music_Generator\\venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\arthu\\Documents\\GAN_Music_Generator\\venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[3], line 99\u001b[0m, in \u001b[0;36mFilesMDCTDataset.__getitem__\u001b[1;34m(self, i)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, i: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m     98\u001b[0m     f, idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpairs[i]\n\u001b[1;32m---> 99\u001b[0m     spec \u001b[38;5;241m=\u001b[39m \u001b[43m_load_mdct_from_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    100\u001b[0m \u001b[43m        \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmdct_feats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mduration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_len\u001b[49m\n\u001b[0;32m    101\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (H=feats, W=feats//2)\u001b[39;00m\n\u001b[0;32m    103\u001b[0m     \u001b[38;5;66;03m# ensure shape (max_feats, mdct_feats//2, 1)\u001b[39;00m\n\u001b[0;32m    104\u001b[0m     H \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_feats\n",
      "Cell \u001b[1;32mIn[3], line 7\u001b[0m, in \u001b[0;36m_load_mdct_from_file\u001b[1;34m(file, idx, rate, feats, duration)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03mMDCT via windowed DCT-IV with 50% overlap.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03mReturns array shape (feats, feats//2) to match your pipeline.\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# load audio robustly\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m audio, _ \u001b[38;5;241m=\u001b[39m \u001b[43m_safe_audio_load\u001b[49m(file, sr\u001b[38;5;241m=\u001b[39mrate, offset\u001b[38;5;241m=\u001b[39midx, duration\u001b[38;5;241m=\u001b[39mduration)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# pad / truncate to exact length\u001b[39;00m\n\u001b[0;32m     10\u001b[0m target_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(rate \u001b[38;5;241m*\u001b[39m duration)\n",
      "\u001b[1;31mNameError\u001b[0m: name '_safe_audio_load' is not defined"
     ]
    }
   ],
   "source": [
    "# %% ------------------------------------ example usage\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # dataset (mirrors your call)\n",
    "    loader = get_files_dataloader(\n",
    "        \"data/REAL_audio/*.wav\",\n",
    "        out_len=3.3,\n",
    "        max_feats=256,\n",
    "        total_seconds=26,\n",
    "        scale=1.0,\n",
    "        batch_size=16,\n",
    "        mdct_feats=256,\n",
    "        rate=10_000,\n",
    "        num_workers=0,\n",
    "        pin_memory=False,\n",
    "    )\n",
    "\n",
    "    # peek a batch to get shape\n",
    "    for batch in loader:\n",
    "        # batch: (B, 1, H, W)\n",
    "        shape = batch.shape  # torch.Size([B, 1, H, W])\n",
    "        break\n",
    "    print(\"batch shape:\", shape)\n",
    "\n",
    "    # build model\n",
    "    model = DDIMTorch(\n",
    "        widths=[128, 128, 128, 128],\n",
    "        block_depth=2,\n",
    "        attention=True,\n",
    "        dim1=shape[2],\n",
    "        dim2=shape[3],\n",
    "        device=\"cuda\",\n",
    "    )\n",
    "\n",
    "    # adapt normalizer (like Keras Normalization.adapt)\n",
    "    model.normalizer.adapt(loader, max_batches=256, device=model.device, show_progress=True)\n",
    "\n",
    "    # optimizer\n",
    "    opt = torch.optim.AdamW(model.network.parameters(), lr=2e-4)\n",
    "\n",
    "    # optional: enable your auxiliary loss terms\n",
    "    model.spec_mod = 0.0\n",
    "    model.dx_mod = 0.0\n",
    "\n",
    "    # tiny training loop sketch\n",
    "    model.train()\n",
    "    for epoch in range(1):\n",
    "        pbar = tqdm(loader, desc=f\"epoch {epoch}\", dynamic_ncols=True, leave=True)\n",
    "        for batch in pbar:\n",
    "            metrics = model.training_step(batch, optimizer=opt)\n",
    "            pbar.set_postfix({\n",
    "                \"n_total\": f\"{metrics['n_total']:.4f}\",\n",
    "                \"d_loss\": f\"{metrics['d_loss']:.4f}\"\n",
    "            })\n",
    "\n",
    "    # sampling example\n",
    "    with torch.no_grad():\n",
    "        gen = model.generate(num_examples=4, shape=(1, shape[2], shape[3]), diffusion_steps=50)\n",
    "        print(\"generated:\", gen.shape)  # (4, 1, H, W)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea722c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch shape: torch.Size([16, 1, 256, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 0:   0%|          | 0/1623 [00:17<?, ?it/s]                     \n"
     ]
    },
    {
     "ename": "AcceleratorError",
     "evalue": "CUDA error: out of memory\nSearch for `cudaErrorMemoryAllocation' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAcceleratorError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 624\u001b[0m\n\u001b[0;32m    622\u001b[0m     pbar \u001b[38;5;241m=\u001b[39m tqdm(loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, dynamic_ncols\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m pbar:\n\u001b[1;32m--> 624\u001b[0m         metrics \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mopt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    625\u001b[0m         pbar\u001b[38;5;241m.\u001b[39mset_postfix({\n\u001b[0;32m    626\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_total\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_total\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    627\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124md_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124md_loss\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    628\u001b[0m         })\n\u001b[0;32m    630\u001b[0m \u001b[38;5;66;03m# sampling example with progress bar\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[10], line 532\u001b[0m, in \u001b[0;36mDDIMTorch.training_step\u001b[1;34m(self, batch, optimizer)\u001b[0m\n\u001b[0;32m    529\u001b[0m noise_rates, signal_rates \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiffusion_schedule(diffusion_times)\n\u001b[0;32m    530\u001b[0m noisy \u001b[38;5;241m=\u001b[39m signal_rates \u001b[38;5;241m*\u001b[39m data \u001b[38;5;241m+\u001b[39m noise_rates \u001b[38;5;241m*\u001b[39m noises\n\u001b[1;32m--> 532\u001b[0m pred_noises, pred_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdenoise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnoisy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoise_rates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignal_rates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    533\u001b[0m noise_loss, noise_spec, noise_dx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_losses(noises, pred_noises)\n\u001b[0;32m    534\u001b[0m data_loss, data_spec, data_dx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_losses(data, pred_data)\n",
      "Cell \u001b[1;32mIn[10], line 488\u001b[0m, in \u001b[0;36mDDIMTorch.denoise\u001b[1;34m(self, noisy_data, noise_rates, signal_rates, training)\u001b[0m\n\u001b[0;32m    486\u001b[0m net \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetwork \u001b[38;5;28;01mif\u001b[39;00m training \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mema_network\n\u001b[0;32m    487\u001b[0m cond \u001b[38;5;241m=\u001b[39m (noise_rates \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mto(noisy_data\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m--> 488\u001b[0m pred_noises \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnoisy_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcond\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    489\u001b[0m pred_data \u001b[38;5;241m=\u001b[39m (noisy_data \u001b[38;5;241m-\u001b[39m noise_rates \u001b[38;5;241m*\u001b[39m pred_noises) \u001b[38;5;241m/\u001b[39m (signal_rates \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-6\u001b[39m)\n\u001b[0;32m    490\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pred_noises, pred_data\n",
      "File \u001b[1;32mc:\\Users\\arthu\\Documents\\GAN_Music_Generator\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\arthu\\Documents\\GAN_Music_Generator\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[10], line 451\u001b[0m, in \u001b[0;36mUNetNoisePredictor.forward\u001b[1;34m(self, x, t_in)\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;66;03m# Up\u001b[39;00m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m up, blocks, skip \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup_convs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup_blocks, \u001b[38;5;28mreversed\u001b[39m(skips[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])):\n\u001b[1;32m--> 451\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[43mup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m                       \u001b[38;5;66;03m# -> (B, ch, H, W)\u001b[39;00m\n\u001b[0;32m    452\u001b[0m     h \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([h, skip], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# -> (B, 2*ch, H, W)\u001b[39;00m\n\u001b[0;32m    453\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m blocks:\n",
      "File \u001b[1;32mc:\\Users\\arthu\\Documents\\GAN_Music_Generator\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\arthu\\Documents\\GAN_Music_Generator\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\arthu\\Documents\\GAN_Music_Generator\\venv\\lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;124;03mRuns the forward pass.\u001b[39;00m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\arthu\\Documents\\GAN_Music_Generator\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\arthu\\Documents\\GAN_Music_Generator\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\arthu\\Documents\\GAN_Music_Generator\\venv\\lib\\site-packages\\torch\\nn\\modules\\conv.py:548\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    547\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 548\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\arthu\\Documents\\GAN_Music_Generator\\venv\\lib\\site-packages\\torch\\nn\\modules\\conv.py:543\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    531\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[0;32m    533\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[0;32m    534\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    541\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[0;32m    542\u001b[0m     )\n\u001b[1;32m--> 543\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    544\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[0;32m    545\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mAcceleratorError\u001b[0m: CUDA error: out of memory\nSearch for `cudaErrorMemoryAllocation' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "# %% =============================== imports\n",
    "import math\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from functools import partial\n",
    "from typing import Tuple\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import librosa\n",
    "\n",
    "try:\n",
    "    from scipy.fft import dct           # modern SciPy\n",
    "except ImportError:\n",
    "    from scipy.fftpack import dct       # fallback\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# Hyperparams (same semantics as your TF version)\n",
    "# ==========================\n",
    "min_signal_rate = 0.02\n",
    "max_signal_rate = 0.95\n",
    "ema = 0.999\n",
    "\n",
    "\n",
    "# %% =============================== losses / metrics (PyTorch)\n",
    "\n",
    "def spectral_norm_diff(pred: torch.Tensor, real: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Difference in 'spectral norm' between batches, analogous to your TF helper.\n",
    "    Supports NCHW (default) or NHWC (will be converted).\n",
    "    \"\"\"\n",
    "    if pred.dim() == 4 and pred.shape[1] not in (1, 2, 3):  # likely NHWC\n",
    "        pred = pred.permute(0, 3, 1, 2).contiguous()\n",
    "        real = real.permute(0, 3, 1, 2).contiguous()\n",
    "    pr = pred.flatten(1)\n",
    "    rr = real.flatten(1)\n",
    "    norm_real = torch.norm(rr, dim=1) + 1e-6\n",
    "    norm_pred = torch.norm(pr, dim=1) + 1e-6\n",
    "    return torch.mean(torch.abs(norm_real - norm_pred) / norm_real)\n",
    "\n",
    "\n",
    "def time_derivative_loss(pred: torch.Tensor, real: torch.Tensor, window: int = 1) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Match finite differences along the 'time' dimension (assume H is time).\n",
    "    Works for NCHW or NHWC (we'll align to NCHW internally).\n",
    "    \"\"\"\n",
    "    if pred.dim() == 4 and pred.shape[1] not in (1, 2, 3):  # NHWC -> NCHW\n",
    "        pred = pred.permute(0, 3, 1, 2)\n",
    "        real = real.permute(0, 3, 1, 2)\n",
    "    real_dx = real[:, :, :-window, :] - real[:, :, window:, :]\n",
    "    pred_dx = pred[:, :, :-window, :] - pred[:, :, window:, :]\n",
    "    return F.mse_loss(pred_dx, real_dx)\n",
    "\n",
    "\n",
    "# %% =============================== robust audio load + MDCT via DCT-IV\n",
    "\n",
    "def _safe_audio_load(path: str, sr: int, offset: float, duration: float) -> Tuple[np.ndarray, int]:\n",
    "    \"\"\"\n",
    "    Try torchaudio -> soundfile -> scipy to load audio robustly.\n",
    "    Returns mono float32 at target sr.\n",
    "    \"\"\"\n",
    "    # 1) torchaudio\n",
    "    try:\n",
    "        import torchaudio\n",
    "        wav, native_sr = torchaudio.load(path)  # shape (C, T)\n",
    "        start = int(offset * native_sr)\n",
    "        end = start + int(duration * native_sr)\n",
    "        wav = wav[:, start:end]\n",
    "        if wav.shape[0] > 1:\n",
    "            wav = wav.mean(0, keepdim=False)    # mono\n",
    "        else:\n",
    "            wav = wav.squeeze(0)\n",
    "        if native_sr != sr:\n",
    "            resamp = torchaudio.transforms.Resample(native_sr, sr)\n",
    "            wav = resamp(wav.unsqueeze(0)).squeeze(0)\n",
    "        return wav.numpy().astype(np.float32), sr\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # 2) soundfile (libsndfile)\n",
    "    try:\n",
    "        import soundfile as sf\n",
    "        y, native_sr = sf.read(path, dtype=\"float32\", always_2d=False)\n",
    "        if y.ndim == 2:\n",
    "            y = y.mean(axis=1)\n",
    "        start = int(offset * native_sr)\n",
    "        end = start + int(duration * native_sr)\n",
    "        y = y[start:end]\n",
    "        if native_sr != sr:\n",
    "            y = librosa.resample(y, orig_sr=native_sr, target_sr=sr)\n",
    "        return y.astype(np.float32), sr\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # 3) scipy (PCM-only)\n",
    "    try:\n",
    "        from scipy.io import wavfile\n",
    "        native_sr, y = wavfile.read(path)\n",
    "        if y.ndim == 2:\n",
    "            y = y.mean(axis=1)\n",
    "        y = y.astype(np.float32)\n",
    "        # normalize if integer PCM range\n",
    "        if y.max() > 1.0 or y.min() < -1.0:\n",
    "            maxv = float(np.max(np.abs(y)) or 1.0)\n",
    "            y = y / maxv\n",
    "        start = int(offset * native_sr)\n",
    "        end = start + int(duration * native_sr)\n",
    "        y = y[start:end]\n",
    "        if native_sr != sr:\n",
    "            y = librosa.resample(y, orig_sr=native_sr, target_sr=sr)\n",
    "        return y.astype(np.float32), sr\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to decode {path}: {e}\")\n",
    "\n",
    "\n",
    "def _load_mdct_from_file(file: str, idx: int, rate: int = 10_000, feats: int = 256, duration: float = 3.3) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    MDCT via windowed DCT-IV with 50% overlap.\n",
    "    Returns array shape (feats, feats//2) to match your pipeline.\n",
    "    \"\"\"\n",
    "    # load audio robustly\n",
    "    audio, _ = _safe_audio_load(file, sr=rate, offset=idx, duration=duration)\n",
    "\n",
    "    # pad / truncate to exact length\n",
    "    target_len = int(rate * duration)\n",
    "    audio_fill = np.zeros(target_len, dtype=np.float32)\n",
    "    audio_fill[:min(len(audio), target_len)] = audio[:target_len]\n",
    "\n",
    "    # framing\n",
    "    N = feats            # MDCT bins (half window)\n",
    "    win_len = 2 * N      # frame length\n",
    "    hop = N              # 50% overlap\n",
    "\n",
    "    # frames: expected shape (win_len, n_frames)\n",
    "    frames = librosa.util.frame(audio_fill, frame_length=win_len, hop_length=hop)\n",
    "    if frames.shape[0] != win_len and frames.shape[1] == win_len:\n",
    "        frames = frames.swapaxes(0, 1)  # enforce (win_len, n_frames)\n",
    "\n",
    "    # sine window along axis=0\n",
    "    window = np.sin(np.pi / win_len * (np.arange(win_len) + 0.5)).astype(np.float32)  # (win_len,)\n",
    "    frames = frames * window[:, None]  # (win_len, n_frames)\n",
    "\n",
    "    # MDCT = DCT-IV along axis=0, keep first N coeffs\n",
    "    mdct = dct(frames, type=4, norm=\"ortho\", axis=0)[:N, :]  # (N, n_frames)\n",
    "\n",
    "    # crop/pad to (feats, feats//2)\n",
    "    H, W = feats, feats // 2\n",
    "    out = np.zeros((H, W), dtype=np.float32)\n",
    "    h = min(H, mdct.shape[0])\n",
    "    w = min(W, mdct.shape[1])\n",
    "    out[:h, :w] = mdct[:h, :w]\n",
    "    return out\n",
    "\n",
    "\n",
    "def _is_decodable(path: str) -> bool:\n",
    "    \"\"\"\n",
    "    Quick pre-check to skip obviously undecodable files.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import torchaudio\n",
    "        torchaudio.info(path)\n",
    "        return True\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        import soundfile as sf\n",
    "        with sf.SoundFile(path):\n",
    "            return True\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        from scipy.io import wavfile\n",
    "        wavfile.read(path)\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "# %% =============================== dataset / dataloader\n",
    "\n",
    "class FilesMDCTDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        glob_location: str,\n",
    "        total_seconds: int = 2,\n",
    "        out_len: float = 3.3,\n",
    "        hop_size: int = 1,\n",
    "        max_feats: int = 2048,\n",
    "        batch_scale: float = 1.0,\n",
    "        rate: int = 10_000,\n",
    "        mdct_feats: int = 256,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        all_files = glob(glob_location, recursive=True)\n",
    "        self.files = [f for f in all_files if _is_decodable(f)]\n",
    "        if len(self.files) == 0:\n",
    "            raise RuntimeError(\"No decodable audio files found. Check backends or path.\")\n",
    "        pairs = []\n",
    "        for s in range(total_seconds):\n",
    "            for f in self.files:\n",
    "                pairs.append((f, s * hop_size))\n",
    "        self.pairs = pairs\n",
    "        self.rate = rate\n",
    "        self.out_len = out_len\n",
    "        self.mdct_feats = mdct_feats\n",
    "        self.max_feats = max_feats\n",
    "        self.scale = batch_scale\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, i: int) -> torch.Tensor:\n",
    "        f, idx = self.pairs[i]\n",
    "        spec = _load_mdct_from_file(\n",
    "            f, idx, rate=self.rate, feats=self.mdct_feats, duration=self.out_len\n",
    "        )  # (H=feats, W=feats//2)\n",
    "\n",
    "        # ensure shape (max_feats, mdct_feats//2, 1)\n",
    "        H = self.max_feats\n",
    "        W = self.mdct_feats // 2\n",
    "        out = np.zeros((H, W), dtype=np.float32)\n",
    "        h = min(H, spec.shape[0])\n",
    "        w = min(W, spec.shape[1])\n",
    "        out[:h, :w] = spec[:h, :w]\n",
    "        out = out[..., None] * self.scale  # (H, W, 1)\n",
    "\n",
    "        # return as CHW for PyTorch (C, H, W)\n",
    "        out = np.transpose(out, (2, 0, 1))  # (1, H, W)\n",
    "        return torch.from_numpy(out)\n",
    "\n",
    "\n",
    "def get_files_dataloader(\n",
    "    glob_location: str,\n",
    "    total_seconds: int = 2,\n",
    "    out_len: float = 3.3,\n",
    "    hop_size: int = 1,\n",
    "    max_feats: int = 256,\n",
    "    batch_size: int = 16,\n",
    "    shuffle_size: int = 1000,  # not used; kept for parity\n",
    "    scale: float = 1.0,\n",
    "    rate: int = 10_000,\n",
    "    mdct_feats: int = 256,\n",
    "    num_workers: int = 0,\n",
    "    pin_memory: bool = False,\n",
    ") -> DataLoader:\n",
    "    ds = FilesMDCTDataset(\n",
    "        glob_location=glob_location,\n",
    "        total_seconds=total_seconds,\n",
    "        out_len=out_len,\n",
    "        hop_size=hop_size,\n",
    "        max_feats=max_feats,\n",
    "        batch_scale=scale,\n",
    "        rate=rate,\n",
    "        mdct_feats=mdct_feats,\n",
    "    )\n",
    "    loader = DataLoader(\n",
    "        ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,  # 0 is safer in notebooks\n",
    "        pin_memory=pin_memory,\n",
    "        drop_last=True,\n",
    "    )\n",
    "    return loader\n",
    "\n",
    "\n",
    "# %% =============================== normalization (adapt with tqdm)\n",
    "\n",
    "class AdaptiveNormalizer(nn.Module):\n",
    "    \"\"\"\n",
    "    Store mean/std estimated from data (like Keras Normalization).\n",
    "    Call .adapt(dataloader, steps=...) once before training.\n",
    "    \"\"\"\n",
    "    def __init__(self, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.register_buffer(\"mean\", torch.zeros(1, 1, 1))\n",
    "        self.register_buffer(\"std\", torch.ones(1, 1, 1))\n",
    "        self.eps = eps\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def adapt(self, dataloader, max_batches: int = 512, device: str = \"cpu\", show_progress: bool = True):\n",
    "        iterator = iter(dataloader)\n",
    "        rng = range(max_batches)\n",
    "        if show_progress:\n",
    "            rng = tqdm(rng, desc=\"adapting normalizer\", dynamic_ncols=True, leave=False)\n",
    "\n",
    "        # streaming Welford\n",
    "        n = 0\n",
    "        mean = 0.0\n",
    "        M2 = 0.0\n",
    "\n",
    "        for _ in rng:\n",
    "            try:\n",
    "                x = next(iterator)  # (B, 1, H, W)\n",
    "            except StopIteration:\n",
    "                break\n",
    "            x = x.to(device).float()\n",
    "            # compute batch mean/var over all elements to match TF's global stats\n",
    "            batch_mean = x.mean().item()\n",
    "            batch_var = x.var(unbiased=False).item()\n",
    "            batch_count = x.numel()\n",
    "\n",
    "            n_total = n + batch_count\n",
    "            delta = batch_mean - mean\n",
    "            mean = mean + delta * (batch_count / max(1, n_total))\n",
    "            M2 = M2 + batch_var * batch_count + (delta ** 2) * n * batch_count / max(1, n_total)\n",
    "            n = n_total\n",
    "\n",
    "        var = M2 / max(1, n - 1)\n",
    "        std = math.sqrt(max(var, 0.0) + self.eps)\n",
    "        self.mean = torch.tensor([[[mean]]], device=device)\n",
    "        self.std = torch.tensor([[[std]]], device=device)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: (B, C, H, W)\n",
    "        return (x - self.mean) / (self.std + self.eps)\n",
    "\n",
    "    def denormalize(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return x * (self.std + self.eps) + self.mean\n",
    "\n",
    "\n",
    "# %% =============================== UNet-like noise predictor (FiLM time conditioning)\n",
    "\n",
    "class FiLMBlock(nn.Module):\n",
    "    def __init__(self, in_ch: int, out_ch: int, t_dim: int, use_attn: bool = False):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n",
    "        self.gn1 = nn.GroupNorm(8, out_ch)\n",
    "        self.to_gamma = nn.Linear(t_dim, out_ch)\n",
    "        self.to_beta = nn.Linear(t_dim, out_ch)\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n",
    "        self.gn2 = nn.GroupNorm(8, out_ch)\n",
    "        self.use_attn = use_attn\n",
    "        if use_attn:\n",
    "            self.q = nn.Conv2d(out_ch, out_ch, 1)\n",
    "            self.k = nn.Conv2d(out_ch, out_ch, 1)\n",
    "            self.v = nn.Conv2d(out_ch, out_ch, 1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, t_emb: torch.Tensor) -> torch.Tensor:\n",
    "        h = self.conv1(x)\n",
    "        h = self.gn1(h)\n",
    "        gamma = self.to_gamma(t_emb).unsqueeze(-1).unsqueeze(-1)  # (B, C, 1, 1)\n",
    "        beta = self.to_beta(t_emb).unsqueeze(-1).unsqueeze(-1)\n",
    "        h = h + gamma\n",
    "        h = F.silu(h)\n",
    "        h = h + beta\n",
    "\n",
    "        h = self.conv2(h)\n",
    "        h = self.gn2(h)\n",
    "        h = F.silu(h)\n",
    "\n",
    "        if self.use_attn:\n",
    "            B, C, H, W = h.shape\n",
    "            q = self.q(h).flatten(2).transpose(1, 2)  # (B, HW, C)\n",
    "            k = self.k(h).flatten(2)                  # (B, C, HW)\n",
    "            v = self.v(h).flatten(2).transpose(1, 2)  # (B, HW, C)\n",
    "            attn = torch.softmax(q @ k / math.sqrt(C), dim=-1)  # (B, HW, HW)\n",
    "            h_attn = attn @ v                                     # (B, HW, C)\n",
    "            h_attn = h_attn.transpose(1, 2).reshape(B, C, H, W)\n",
    "            h = h + h_attn\n",
    "        return h\n",
    "\n",
    "\n",
    "class TimeEmbed(nn.Module):\n",
    "    def __init__(self, dim: int = 128):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(1, dim)\n",
    "        self.fc2 = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, t: torch.Tensor) -> torch.Tensor:\n",
    "        # t: (B,1,1,1) or (B,1)\n",
    "        if t.dim() == 4:\n",
    "            t = t.view(t.size(0), 1)\n",
    "        x = F.silu(self.fc1(t))\n",
    "        x = F.silu(self.fc2(x))\n",
    "        return x  # (B, dim)\n",
    "\n",
    "\n",
    "class UNetNoisePredictor(nn.Module):\n",
    "    def __init__(self, widths, block_depth, attention=False, dim1=256, dim2=128, t_dim=128):\n",
    "        super().__init__()\n",
    "        self.dim1, self.dim2 = dim1, dim2\n",
    "        self.t_embed = TimeEmbed(t_dim)\n",
    "\n",
    "        chs = widths\n",
    "        in_ch = 1\n",
    "\n",
    "        # -------- Down path\n",
    "        self.down_blocks = nn.ModuleList()\n",
    "        self.downs = nn.ModuleList()\n",
    "        for i, ch in enumerate(chs):\n",
    "            blocks = nn.ModuleList()\n",
    "            for d in range(block_depth):\n",
    "                blocks.append(FiLMBlock(in_ch, ch, t_dim, use_attn=attention and (i >= len(chs)//2)))\n",
    "                in_ch = ch  # after first block, in_ch == ch, so subsequent are ch->ch\n",
    "            self.down_blocks.append(blocks)\n",
    "            if i < len(chs) - 1:\n",
    "                self.downs.append(nn.Conv2d(ch, ch, 3, stride=2, padding=1))\n",
    "\n",
    "        # -------- Mid\n",
    "        self.mid = FiLMBlock(chs[-1], chs[-1], t_dim, use_attn=attention)\n",
    "\n",
    "        # -------- Up path\n",
    "        self.up_convs = nn.ModuleList()\n",
    "        self.up_blocks = nn.ModuleList()\n",
    "        for i, ch in reversed(list(enumerate(chs[:-1]))):\n",
    "            # upsample + reduce channels from chs[i+1] -> ch\n",
    "            self.up_convs.append(nn.Sequential(\n",
    "                nn.Upsample(scale_factor=2, mode=\"nearest\"),\n",
    "                nn.Conv2d(chs[i+1], ch, 3, padding=1),\n",
    "            ))\n",
    "\n",
    "            # First block sees concat([h, skip]) => 2*ch in, ch out\n",
    "            blocks = nn.ModuleList()\n",
    "            if block_depth >= 1:\n",
    "                blocks.append(FiLMBlock(2 * ch, ch, t_dim, use_attn=attention and (i >= len(chs)//2)))\n",
    "            # Remaining blocks are ch -> ch\n",
    "            for _ in range(block_depth - 1):\n",
    "                blocks.append(FiLMBlock(ch, ch, t_dim, use_attn=attention and (i >= len(chs)//2)))\n",
    "            self.up_blocks.append(blocks)\n",
    "\n",
    "        self.out = nn.Conv2d(chs[0], 1, 1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, t_in: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: (B,1,H,W); t_in: (B,1,1,1) or (B,1) with values in [0,1]\n",
    "        \"\"\"\n",
    "        t_emb = self.t_embed(t_in)\n",
    "\n",
    "        # Down\n",
    "        skips = []\n",
    "        h = x\n",
    "        for i, blocks in enumerate(self.down_blocks):\n",
    "            for blk in blocks:\n",
    "                h = blk(h, t_emb)\n",
    "            skips.append(h)\n",
    "            if i < len(self.downs):\n",
    "                h = self.downs[i](h)\n",
    "\n",
    "        # Mid\n",
    "        h = self.mid(h, t_emb)\n",
    "\n",
    "        # Up\n",
    "        for up, blocks, skip in zip(self.up_convs, self.up_blocks, reversed(skips[:-1])):\n",
    "            h = up(h)                       # -> (B, ch, H, W)\n",
    "            h = torch.cat([h, skip], dim=1) # -> (B, 2*ch, H, W)\n",
    "            for blk in blocks:\n",
    "                h = blk(h, t_emb)           # first block expects 2*ch in, rest ch in\n",
    "\n",
    "        return self.out(h)\n",
    "\n",
    "\n",
    "# %% =============================== DDIM model (PyTorch)\n",
    "\n",
    "class DDIMTorch(nn.Module):\n",
    "    def __init__(self, widths, block_depth, attention=False, dim1=256, dim2=128, device=\"cuda\"):\n",
    "        super().__init__()\n",
    "        self.device = torch.device(device if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.normalizer = AdaptiveNormalizer()\n",
    "        self.network = UNetNoisePredictor(widths, block_depth, attention=attention, dim1=dim1, dim2=dim2).to(self.device)\n",
    "        self.ema_network = UNetNoisePredictor(widths, block_depth, attention=attention, dim1=dim1, dim2=dim2).to(self.device)\n",
    "        self.ema_network.load_state_dict(self.network.state_dict())\n",
    "        self.spec_mod = 0.0\n",
    "        self.dx_mod = 0.0\n",
    "\n",
    "        self.mse = nn.MSELoss()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def diffusion_schedule(self, diffusion_times: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        # diffusion_times: (B,1,1,1) in [0,1]\n",
    "        start_angle = math.acos(max_signal_rate)\n",
    "        end_angle = math.acos(min_signal_rate)\n",
    "        diffusion_angles = start_angle + diffusion_times * (end_angle - start_angle)\n",
    "        signal_rates = torch.cos(diffusion_angles)\n",
    "        noise_rates = torch.sin(diffusion_angles)\n",
    "        return noise_rates, signal_rates\n",
    "\n",
    "    def denoise(self, noisy_data: torch.Tensor, noise_rates: torch.Tensor, signal_rates: torch.Tensor, training: bool = True):\n",
    "        net = self.network if training else self.ema_network\n",
    "        cond = (noise_rates ** 2).to(noisy_data.dtype)\n",
    "        pred_noises = net(noisy_data, cond)\n",
    "        pred_data = (noisy_data - noise_rates * pred_noises) / (signal_rates + 1e-6)\n",
    "        return pred_noises, pred_data\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def reverse_diffusion(self, initial_noise: torch.Tensor, diffusion_steps: int) -> torch.Tensor:\n",
    "        B = initial_noise.size(0)\n",
    "        step_size = 1.0 / diffusion_steps\n",
    "        next_noisy = initial_noise\n",
    "        for step in tqdm(range(diffusion_steps), desc=\"sampling\", dynamic_ncols=True, leave=False):\n",
    "            noisy = next_noisy\n",
    "            t = torch.ones((B, 1, 1, 1), device=self.device) - step * step_size\n",
    "            noise_rates, signal_rates = self.diffusion_schedule(t)\n",
    "            pred_noises, pred_data = self.denoise(noisy, noise_rates, signal_rates, training=False)\n",
    "            t_next = t - step_size\n",
    "            next_noise_rates, next_signal_rates = self.diffusion_schedule(t_next)\n",
    "            next_noisy = next_signal_rates * pred_data + next_noise_rates * pred_noises\n",
    "        return pred_data\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, num_examples: int, shape: Tuple[int, int, int], diffusion_steps: int) -> torch.Tensor:\n",
    "        # shape: (C,H,W)\n",
    "        initial_noise = torch.randn((num_examples, *shape), device=self.device)\n",
    "        generated = self.reverse_diffusion(initial_noise, diffusion_steps)\n",
    "        denorm = self.normalizer.denormalize(generated)\n",
    "        return torch.clamp(denorm, -128.0, 128.0)\n",
    "\n",
    "    def _get_losses(self, y_true: torch.Tensor, y_pred: torch.Tensor):\n",
    "        l = self.mse(y_pred, y_true)\n",
    "        s = spectral_norm_diff(y_pred, y_true)\n",
    "        d = time_derivative_loss(y_pred, y_true)\n",
    "        return l, s, d\n",
    "\n",
    "    def training_step(self, batch: torch.Tensor, optimizer: torch.optim.Optimizer):\n",
    "        batch = batch.to(self.device).float()  # (B,1,H,W)\n",
    "\n",
    "        data = self.normalizer(batch)\n",
    "        noises = torch.randn_like(data)\n",
    "\n",
    "        B = data.size(0)\n",
    "        diffusion_times = torch.rand((B, 1, 1, 1), device=self.device)\n",
    "        noise_rates, signal_rates = self.diffusion_schedule(diffusion_times)\n",
    "        noisy = signal_rates * data + noise_rates * noises\n",
    "\n",
    "        pred_noises, pred_data = self.denoise(noisy, noise_rates, signal_rates, training=True)\n",
    "        noise_loss, noise_spec, noise_dx = self._get_losses(noises, pred_noises)\n",
    "        data_loss, data_spec, data_dx = self._get_losses(data, pred_data)\n",
    "\n",
    "        total_noise_loss = noise_loss + self.spec_mod * noise_spec + self.dx_mod * noise_dx\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        total_noise_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # EMA\n",
    "        with torch.no_grad():\n",
    "            for p, q in zip(self.network.parameters(), self.ema_network.parameters()):\n",
    "                q.mul_(ema).add_(p, alpha=1.0 - ema)\n",
    "\n",
    "        metrics = {\n",
    "            \"n_loss\": noise_loss.detach().item(),\n",
    "            \"d_loss\": data_loss.detach().item(),\n",
    "            \"n_spec\": noise_spec.detach().item(),\n",
    "            \"d_spec\": data_spec.detach().item(),\n",
    "            \"n_dx\": noise_dx.detach().item(),\n",
    "            \"d_dx\": data_dx.detach().item(),\n",
    "            \"n_total\": total_noise_loss.detach().item(),\n",
    "            \"d_total\": (data_loss + self.spec_mod * data_spec + self.dx_mod * data_dx).detach().item(),\n",
    "        }\n",
    "        return metrics\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def eval_step(self, batch: torch.Tensor):\n",
    "        batch = batch.to(self.device).float()\n",
    "        data = self.normalizer(batch)\n",
    "        noises = torch.randn_like(data)\n",
    "\n",
    "        B = data.size(0)\n",
    "        diffusion_times = torch.rand((B, 1, 1, 1), device=self.device)\n",
    "        noise_rates, signal_rates = self.diffusion_schedule(diffusion_times)\n",
    "        noisy = signal_rates * data + noise_rates * noises\n",
    "\n",
    "        pred_noises, pred_data = self.denoise(noisy, noise_rates, signal_rates, training=False)\n",
    "        noise_loss = self.mse(noises, pred_noises)\n",
    "        data_loss = self.mse(data, pred_data)\n",
    "        return {\"n_loss\": noise_loss.item(), \"d_loss\": data_loss.item()}\n",
    "\n",
    "\n",
    "# %% =============================== example usage (notebook-friendly)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # dataset\n",
    "    loader = get_files_dataloader(\n",
    "        \"data/REAL_audio/*.wav\",   # adjust your glob if needed\n",
    "        out_len=3.3,\n",
    "        max_feats=256,\n",
    "        total_seconds=26,\n",
    "        scale=1.0,\n",
    "        batch_size=16,\n",
    "        mdct_feats=256,\n",
    "        rate=10_000,\n",
    "        num_workers=0,             # notebook-safe\n",
    "        pin_memory=False,          # notebook-safe\n",
    "    )\n",
    "\n",
    "    # peek a batch to get shape\n",
    "    for batch in loader:\n",
    "        shape = batch.shape  # (B, 1, H, W)\n",
    "        break\n",
    "    print(\"batch shape:\", shape)\n",
    "\n",
    "    # model\n",
    "    model = DDIMTorch(\n",
    "        widths=[128, 128, 128, 128],\n",
    "        block_depth=2,\n",
    "        attention=True,\n",
    "        dim1=shape[2],\n",
    "        dim2=shape[3],\n",
    "        device=\"cuda\",\n",
    "    )\n",
    "\n",
    "    # adapt normalizer with progress bar\n",
    "    model.normalizer.adapt(loader, max_batches=256, device=str(model.device), show_progress=True)\n",
    "\n",
    "    # optimizer\n",
    "    opt = torch.optim.AdamW(model.network.parameters(), lr=2e-4)\n",
    "\n",
    "    # optional aux terms\n",
    "    model.spec_mod = 0.0\n",
    "    model.dx_mod = 0.0\n",
    "\n",
    "    # tiny training loop with tqdm\n",
    "    model.train()\n",
    "    for epoch in range(1):\n",
    "        pbar = tqdm(loader, desc=f\"epoch {epoch}\", dynamic_ncols=True, leave=True)\n",
    "        for batch in pbar:\n",
    "            metrics = model.training_step(batch, optimizer=opt)\n",
    "            pbar.set_postfix({\n",
    "                \"n_total\": f\"{metrics['n_total']:.4f}\",\n",
    "                \"d_loss\": f\"{metrics['d_loss']:.4f}\",\n",
    "            })\n",
    "\n",
    "    # sampling example with progress bar\n",
    "    with torch.no_grad():\n",
    "        gen = model.generate(num_examples=4, shape=(1, shape[2], shape[3]), diffusion_steps=50)\n",
    "        print(\"generated:\", gen.shape)  # (4, 1, H, W)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068b3c88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
