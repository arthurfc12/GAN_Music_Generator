{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "859c2e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Training windows: ~2.97s | Mel: 128x256\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# Real-only Music GAN (WGAN-GP) — Train on REAL_AUDIO only and generate new music\n",
    "# Paste these cells into your `music_gan_realism.ipynb` (or run as-is in a new notebook).\n",
    "# This version removes any need for AI/MP3 folders and fixes glob + typos.\n",
    "\n",
    "# %%\n",
    "# !pip install torch torchaudio librosa soundfile numpy scikit-learn tqdm --quiet\n",
    "\n",
    "# %%\n",
    "import os, random, math, io, warnings\n",
    "import glob  # <-- use module form; call as glob.glob(...)\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "import librosa.feature\n",
    "import librosa.display\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# ----------------------------\n",
    "# Config (edit as needed)\n",
    "# ----------------------------\n",
    "SEED = 17\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "REAL_AUDIO_DIR = \"data/REAL_audio\"  # <-- only this is required now\n",
    "\n",
    "SR = 22050\n",
    "N_FFT = 1024\n",
    "HOP = 256\n",
    "WIN = 1024\n",
    "N_MELS = 128\n",
    "FMIN = 20\n",
    "FMAX = 8000\n",
    "\n",
    "# frames per training window (mel time steps)\n",
    "FRAMES = 256  # ≈ FRAMES*HOP/SR seconds (here ~2.97 s); raise if you have VRAM\n",
    "WINDOW_SEC = FRAMES * HOP / SR\n",
    "\n",
    "BATCH = 32\n",
    "EPOCHS = 50\n",
    "LR_G = 2e-4\n",
    "LR_D = 2e-4\n",
    "BETAS = (0.5, 0.9)\n",
    "LAMBDA_GP = 10.0\n",
    "N_CRITIC = 5  # D steps per G step\n",
    "\n",
    "# Snapshot & sampling controls (to reduce between-epoch stalls)\n",
    "SAVE_EVERY_EPOCHS = 5      # set to 1 to save every epoch\n",
    "SAMPLES_PER_SNAPSHOT = 4   # number of WAVs to render per snapshot\n",
    "SAMPLE_EVERY_EPOCHS = 5    # set to 1 to sample every epoch; None to disable\n",
    "GRIFFIN_LIM_ITERS = 24     # 64 sounds a bit nicer but is much slower\n",
    "\n",
    "SAVE_DIR = \"runs/real_only_gan\"\n",
    "SAMPLES_DIR = f\"{SAVE_DIR}/samples\"\n",
    "CKPT_DIR = f\"{SAVE_DIR}/ckpts\"\n",
    "os.makedirs(SAMPLES_DIR, exist_ok=True)\n",
    "os.makedirs(CKPT_DIR, exist_ok=True)\n",
    "\n",
    "# ----------------------------\n",
    "# Utils: audio <-> mel\n",
    "# ----------------------------\n",
    "EPS = 1e-7\n",
    "\n",
    "def wav_to_logmel(y: np.ndarray) -> np.ndarray:\n",
    "    S = librosa.feature.melspectrogram(\n",
    "        y=y, sr=SR, n_fft=N_FFT, hop_length=HOP, win_length=WIN,\n",
    "        n_mels=N_MELS, power=2.0, fmin=FMIN, fmax=FMAX,\n",
    "    )\n",
    "    S = np.maximum(S, EPS)\n",
    "    logS = np.log(S)\n",
    "    return logS  # natural log of power mel\n",
    "\n",
    "def logmel_to_wav(logS: np.ndarray, length: int | None = None) -> np.ndarray:\n",
    "    S = np.exp(logS)  # back to power mel\n",
    "    y = librosa.feature.inverse.mel_to_audio(\n",
    "        M=S, sr=SR, n_fft=N_FFT, hop_length=HOP, win_length=WIN,\n",
    "        fmin=FMIN, fmax=FMAX, power=2.0, n_iter=GRIFFIN_LIM_ITERS\n",
    "    )\n",
    "    if length is not None and len(y) > length:\n",
    "        y = y[:length]\n",
    "    return y\n",
    "\n",
    "# ----------------------------\n",
    "# Dataset — REAL audio only\n",
    "# ----------------------------\n",
    "class RealMelDataset(Dataset):\n",
    "    def __init__(self, root: str, frames: int = FRAMES):\n",
    "        exts = (\".wav\", \".flac\", \".mp3\", \".ogg\", \".m4a\", \".aiff\", \".aif\")\n",
    "        self.files = [p for p in glob.glob(os.path.join(root, \"**\", \"*\"), recursive=True)\n",
    "                      if p.lower().endswith(exts)]\n",
    "        self.frames = frames\n",
    "        self.valid = []\n",
    "        for p in self.files:\n",
    "            try:\n",
    "                info = sf.info(p)\n",
    "                if info.samplerate <= 0 or info.frames < SR * 2:\n",
    "                    continue\n",
    "                # quick probe: try reading a tiny slice\n",
    "                y, _ = librosa.load(p, sr=SR, mono=True, duration=1.0)\n",
    "                if y is None or len(y) == 0:\n",
    "                    continue\n",
    "                self.valid.append(p)\n",
    "            except Exception:\n",
    "                continue\n",
    "        if not self.valid:\n",
    "            raise RuntimeError(f\"No valid audio found under {root}. Checked {len(self.files)} files.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.valid)\n",
    "\n",
    "    def _load_segment(self, path: str):\n",
    "        y, sr = librosa.load(path, sr=SR, mono=True)\n",
    "        target_len = self.frames * HOP + WIN\n",
    "        if len(y) < target_len:\n",
    "            y = np.pad(y, (0, target_len - len(y)))\n",
    "        max_start = max(0, len(y) - target_len)\n",
    "        start = 0 if max_start == 0 else np.random.randint(0, max_start + 1)\n",
    "        segment = y[start:start + target_len]\n",
    "        return segment\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # robust loop: if a file breaks in a worker, try a different one\n",
    "        for _ in range(5):\n",
    "            path = self.valid[idx % len(self.valid)]\n",
    "            try:\n",
    "                segment = self._load_segment(path)\n",
    "                logmel = wav_to_logmel(segment)\n",
    "                if logmel.shape[1] < self.frames:\n",
    "                    pad_w = self.frames - logmel.shape[1]\n",
    "                    logmel = np.pad(logmel, ((0, 0), (0, pad_w)), mode='edge')\n",
    "                logmel = logmel[:, :self.frames]\n",
    "                x = torch.from_numpy(logmel).float().unsqueeze(0)\n",
    "                return x\n",
    "            except Exception:\n",
    "                # pick a random fallback index\n",
    "                idx = np.random.randint(0, len(self.valid))\n",
    "                continue\n",
    "        # last resort: return zeros to avoid crashing the worker\n",
    "        x = torch.zeros(1, N_MELS, self.frames, dtype=torch.float32)\n",
    "        return x\n",
    "\n",
    "# ----------------------------\n",
    "# Models: discriminator & generator on mel space\n",
    "# ----------------------------\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        ch = [1, 32, 64, 128, 256]\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(ch[0], ch[1], 4, 2, 1), nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(ch[1], ch[2], 4, 2, 1), nn.InstanceNorm2d(ch[2], affine=True), nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(ch[2], ch[3], 4, 2, 1), nn.InstanceNorm2d(ch[3], affine=True), nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(ch[3], ch[4], 4, 2, 1), nn.InstanceNorm2d(ch[4], affine=True), nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "        self.head = nn.Conv2d(ch[4], 1, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.net(x)\n",
    "        h = F.adaptive_avg_pool2d(h, (1, 1))\n",
    "        score = self.head(h).view(x.size(0))\n",
    "        return score\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim=128, out_h=N_MELS, out_w=FRAMES):\n",
    "        super().__init__()\n",
    "        self.out_h = out_h\n",
    "        self.out_w = out_w\n",
    "        base = 256\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(z_dim, base*8), nn.ReLU(True),\n",
    "            nn.Linear(base*8, base*16), nn.ReLU(True),\n",
    "        )\n",
    "        self.start_h, self.start_w = max(1, out_h//16), max(1, out_w//16)\n",
    "        self.proj = nn.Linear(base*16, 256 * self.start_h * self.start_w)\n",
    "        self.up = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1), nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1), nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(64, 32, 4, 2, 1), nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(32, 16, 4, 2, 1), nn.ReLU(True),\n",
    "            nn.Conv2d(16, 1, 3, 1, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        h = self.fc(z)\n",
    "        h = self.proj(h)\n",
    "        h = h.view(z.size(0), 256, self.start_h, self.start_w)\n",
    "        x = self.up(h)\n",
    "        # crop/resize to exact target if off-by-one due to strides\n",
    "        if x.size(2) != self.out_h or x.size(3) != self.out_w:\n",
    "            x = F.interpolate(x, size=(self.out_h, self.out_w), mode=\"bilinear\", align_corners=False)\n",
    "        return x\n",
    "\n",
    "# ----------------------------\n",
    "# WGAN-GP training (REAL only)\n",
    "# ----------------------------\n",
    "\n",
    "def gradient_penalty(D, real, fake):\n",
    "    bsz = real.size(0)\n",
    "    eps = torch.rand(bsz, 1, 1, 1, device=real.device)\n",
    "    inter = eps * real + (1 - eps) * fake\n",
    "    inter.requires_grad_(True)\n",
    "    d_inter = D(inter)\n",
    "    grads = torch.autograd.grad(\n",
    "        outputs=d_inter, inputs=inter,\n",
    "        grad_outputs=torch.ones_like(d_inter),\n",
    "        create_graph=True, retain_graph=True, only_inputs=True\n",
    "    )[0]\n",
    "    grads = grads.view(bsz, -1)\n",
    "    gp = ((grads.norm(2, dim=1) - 1.0) ** 2).mean()\n",
    "    return gp\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample_mels(G, n=8, z_dim=128):\n",
    "    G.eval()\n",
    "    z = torch.randn(n, z_dim, device=DEVICE)\n",
    "    mels = G(z).cpu().numpy()  # [n,1,N_MELS,FRAMES]\n",
    "    return mels\n",
    "\n",
    "@torch.no_grad()\n",
    "def save_audio_samples(G, step_tag: str, n=SAMPLES_PER_SNAPSHOT, z_dim=128):\n",
    "    mels = sample_mels(G, n=n, z_dim=z_dim)\n",
    "    for i, mel in enumerate(mels):\n",
    "        logmel = mel[0]\n",
    "        y = logmel_to_wav(logmel)\n",
    "        y = np.clip(y, -1.0, 1.0)\n",
    "        sf.write(f\"{SAMPLES_DIR}/sample_{step_tag}_{i:02d}.wav\", y, SR)\n",
    "\n",
    "\n",
    "def train_real_only_wgan_gp(resume_path: str | None = None):\n",
    "    torch.manual_seed(SEED); np.random.seed(SEED); random.seed(SEED)\n",
    "    ds = RealMelDataset(REAL_AUDIO_DIR, frames=FRAMES)\n",
    "    dl = DataLoader(ds, batch_size=BATCH, shuffle=True, num_workers=0, drop_last=True, pin_memory=False, persistent_workers=False)\n",
    "\n",
    "    D = Discriminator().to(DEVICE)\n",
    "    G = Generator(z_dim=128, out_h=N_MELS, out_w=FRAMES).to(DEVICE)\n",
    "\n",
    "    if resume_path and os.path.exists(resume_path):\n",
    "        G.load_state_dict(torch.load(resume_path, map_location=DEVICE))\n",
    "        print(f\"[Resume] Loaded generator weights from {resume_path}\")\n",
    "\n",
    "    optD = torch.optim.Adam(D.parameters(), lr=LR_D, betas=BETAS)\n",
    "    optG = torch.optim.Adam(G.parameters(), lr=LR_G, betas=BETAS)\n",
    "\n",
    "    global_step = 0\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        pbar = tqdm(dl, desc=f\"Epoch {epoch}/{EPOCHS}\")\n",
    "        for real in pbar:\n",
    "            real = real.to(DEVICE)\n",
    "            bsz = real.size(0)\n",
    "            # --------------------\n",
    "            # (1) Update D N_CRITIC times\n",
    "            # --------------------\n",
    "            for _ in range(N_CRITIC):\n",
    "                z = torch.randn(bsz, 128, device=DEVICE)\n",
    "                fake = G(z).detach()\n",
    "                d_real = D(real)\n",
    "                d_fake = D(fake)\n",
    "                gp = gradient_penalty(D, real, fake)\n",
    "                lossD = (d_fake - d_real).mean() + LAMBDA_GP * gp\n",
    "                optD.zero_grad(set_to_none=True)\n",
    "                lossD.backward()\n",
    "                optD.step()\n",
    "\n",
    "            # --------------------\n",
    "            # (2) Update G once\n",
    "            # --------------------\n",
    "            z = torch.randn(bsz, 128, device=DEVICE)\n",
    "            fake = G(z)\n",
    "            lossG = -D(fake).mean()\n",
    "            optG.zero_grad(set_to_none=True)\n",
    "            lossG.backward()\n",
    "            optG.step()\n",
    "\n",
    "            global_step += 1\n",
    "            if global_step % 100 == 0:\n",
    "                pbar.set_postfix({\"lossD\": float(lossD.item()), \"lossG\": float(lossG.item())})\n",
    "        # end epoch — heavy I/O moved behind frequency gates\n",
    "        if (epoch % SAVE_EVERY_EPOCHS) == 0 or epoch == EPOCHS:\n",
    "            torch.save(G.state_dict(), f\"{CKPT_DIR}/G_epoch{epoch:03d}.pt\")\n",
    "            torch.save(D.state_dict(), f\"{CKPT_DIR}/D_epoch{epoch:03d}.pt\")\n",
    "        if SAMPLE_EVERY_EPOCHS is not None and ((epoch % SAMPLE_EVERY_EPOCHS) == 0 or epoch == EPOCHS):\n",
    "            save_audio_samples(G, step_tag=f\"e{epoch}\", n=SAMPLES_PER_SNAPSHOT)\n",
    "    return G, D\n",
    "\n",
    "# %% [markdown]\n",
    "# ---- Quickstart ----\n",
    "# 1) Put your real audio under REAL_AUDIO_DIR (any extension supported by soundfile/librosa)\n",
    "# 2) Run: G, D = train_real_only_wgan_gp()\n",
    "# 3) Generate new music snippets at any time: save_audio_samples(G, step_tag=\"manual\", n=8)\n",
    "#    Files will appear under SAMPLES_DIR.\n",
    "# 4) (Optional) Resume training: G, D = train_real_only_wgan_gp(resume_path=\"runs/real_only_gan/ckpts/G_epoch010.pt\")\n",
    "\n",
    "# %%\n",
    "if __name__ == \"__main__\":\n",
    "    print(f\"Device: {DEVICE}\")\n",
    "    print(f\"Training windows: ~{WINDOW_SEC:.2f}s | Mel: {N_MELS}x{FRAMES}\")\n",
    "    # Uncomment to train from script:\n",
    "    # G, D = train_real_only_wgan_gp()\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fcdfda4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: 100%|██████████| 31/31 [00:24<00:00,  1.25it/s]\n",
      "Epoch 2/50: 100%|██████████| 31/31 [00:24<00:00,  1.24it/s]\n",
      "Epoch 3/50: 100%|██████████| 31/31 [00:24<00:00,  1.24it/s]\n",
      "Epoch 4/50: 100%|██████████| 31/31 [00:25<00:00,  1.23it/s, lossD=1.46, lossG=-0.00453]\n",
      "Epoch 5/50: 100%|██████████| 31/31 [00:24<00:00,  1.24it/s]\n",
      "Epoch 6/50: 100%|██████████| 31/31 [00:24<00:00,  1.24it/s]\n",
      "Epoch 7/50: 100%|██████████| 31/31 [00:25<00:00,  1.23it/s, lossD=-8.99, lossG=3.98]\n",
      "Epoch 8/50: 100%|██████████| 31/31 [00:24<00:00,  1.24it/s]\n",
      "Epoch 9/50: 100%|██████████| 31/31 [00:24<00:00,  1.24it/s]\n",
      "Epoch 10/50: 100%|██████████| 31/31 [00:25<00:00,  1.24it/s, lossD=-23.4, lossG=12.3]\n",
      "Epoch 11/50: 100%|██████████| 31/31 [00:25<00:00,  1.23it/s]\n",
      "Epoch 12/50: 100%|██████████| 31/31 [00:25<00:00,  1.24it/s]\n",
      "Epoch 13/50: 100%|██████████| 31/31 [00:25<00:00,  1.23it/s, lossD=-35.7, lossG=16.9]\n",
      "Epoch 14/50: 100%|██████████| 31/31 [00:24<00:00,  1.24it/s]\n",
      "Epoch 15/50: 100%|██████████| 31/31 [00:24<00:00,  1.24it/s]\n",
      "Epoch 16/50: 100%|██████████| 31/31 [00:24<00:00,  1.24it/s]\n",
      "Epoch 17/50: 100%|██████████| 31/31 [00:25<00:00,  1.23it/s, lossD=-48.1, lossG=27.4]\n",
      "Epoch 18/50: 100%|██████████| 31/31 [00:24<00:00,  1.24it/s]\n",
      "Epoch 19/50: 100%|██████████| 31/31 [00:24<00:00,  1.24it/s]\n",
      "Epoch 20/50: 100%|██████████| 31/31 [00:25<00:00,  1.24it/s, lossD=-57.6, lossG=28.6]\n",
      "Epoch 21/50: 100%|██████████| 31/31 [00:25<00:00,  1.24it/s]\n",
      "Epoch 22/50: 100%|██████████| 31/31 [00:24<00:00,  1.24it/s]\n",
      "Epoch 23/50: 100%|██████████| 31/31 [00:25<00:00,  1.24it/s, lossD=-73.6, lossG=37.8]\n",
      "Epoch 24/50: 100%|██████████| 31/31 [00:24<00:00,  1.24it/s]\n",
      "Epoch 25/50: 100%|██████████| 31/31 [00:25<00:00,  1.24it/s]\n",
      "Epoch 26/50: 100%|██████████| 31/31 [00:25<00:00,  1.23it/s, lossD=-78.2, lossG=48.8]\n",
      "Epoch 27/50: 100%|██████████| 31/31 [00:24<00:00,  1.24it/s]\n",
      "Epoch 28/50: 100%|██████████| 31/31 [00:24<00:00,  1.24it/s]\n",
      "Epoch 29/50: 100%|██████████| 31/31 [00:24<00:00,  1.24it/s]\n",
      "Epoch 30/50: 100%|██████████| 31/31 [00:25<00:00,  1.24it/s, lossD=-97.2, lossG=58.6]\n",
      "Epoch 31/50: 100%|██████████| 31/31 [00:25<00:00,  1.24it/s]\n",
      "Epoch 32/50: 100%|██████████| 31/31 [00:24<00:00,  1.24it/s]\n",
      "Epoch 33/50: 100%|██████████| 31/31 [00:25<00:00,  1.23it/s, lossD=-119, lossG=69]\n",
      "Epoch 34/50: 100%|██████████| 31/31 [00:24<00:00,  1.24it/s]\n",
      "Epoch 35/50: 100%|██████████| 31/31 [00:24<00:00,  1.24it/s]\n",
      "Epoch 36/50: 100%|██████████| 31/31 [00:25<00:00,  1.23it/s, lossD=-136, lossG=80.2]\n",
      "Epoch 37/50: 100%|██████████| 31/31 [00:24<00:00,  1.24it/s]\n",
      "Epoch 38/50: 100%|██████████| 31/31 [00:24<00:00,  1.24it/s]\n",
      "Epoch 39/50: 100%|██████████| 31/31 [00:25<00:00,  1.23it/s, lossD=-173, lossG=97.9]\n",
      "Epoch 40/50: 100%|██████████| 31/31 [00:24<00:00,  1.24it/s]\n",
      "Epoch 41/50: 100%|██████████| 31/31 [00:25<00:00,  1.23it/s]\n",
      "Epoch 42/50: 100%|██████████| 31/31 [00:25<00:00,  1.23it/s, lossD=-166, lossG=80]\n",
      "Epoch 43/50: 100%|██████████| 31/31 [00:24<00:00,  1.24it/s]\n",
      "Epoch 44/50: 100%|██████████| 31/31 [00:25<00:00,  1.24it/s]\n",
      "Epoch 45/50: 100%|██████████| 31/31 [00:25<00:00,  1.24it/s]\n",
      "Epoch 46/50: 100%|██████████| 31/31 [00:25<00:00,  1.23it/s, lossD=-170, lossG=105]\n",
      "Epoch 47/50: 100%|██████████| 31/31 [00:24<00:00,  1.24it/s]\n",
      "Epoch 48/50: 100%|██████████| 31/31 [00:24<00:00,  1.24it/s]\n",
      "Epoch 49/50: 100%|██████████| 31/31 [00:25<00:00,  1.23it/s, lossD=-151, lossG=77.2]\n",
      "Epoch 50/50: 100%|██████████| 31/31 [00:25<00:00,  1.24it/s]\n"
     ]
    }
   ],
   "source": [
    "G, D = train_real_only_wgan_gp()\n",
    "save_audio_samples(G, step_tag=\"manual\", n=8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957b2b9b",
   "metadata": {},
   "source": [
    "# Model -> DDIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df013924",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import librosa \n",
    "from glob import glob\n",
    "\n",
    "import random\n",
    "from functools import partial\n",
    "import warnings\n",
    "import IPython.display as ipd\n",
    "from tensorflow.keras import mixed_precision\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1cf02f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 4050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())   # should be True\n",
    "print(torch.cuda.get_device_name(0))  # GPU name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dcca64a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/REAL_audio\\\\blues.00000.wav',\n",
       " 'data/REAL_audio\\\\blues.00001.wav',\n",
       " 'data/REAL_audio\\\\blues.00002.wav',\n",
       " 'data/REAL_audio\\\\blues.00003.wav',\n",
       " 'data/REAL_audio\\\\blues.00004.wav',\n",
       " 'data/REAL_audio\\\\blues.00005.wav',\n",
       " 'data/REAL_audio\\\\blues.00006.wav',\n",
       " 'data/REAL_audio\\\\blues.00007.wav',\n",
       " 'data/REAL_audio\\\\blues.00008.wav',\n",
       " 'data/REAL_audio\\\\blues.00009.wav']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "music_files = glob(\"data/REAL_audio/**/*.wav\", recursive=True)\n",
    "music_files[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90c0327e",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_signal_rate = 0.02\n",
    "max_signal_rate = 0.95\n",
    "ema = 0.999\n",
    "\n",
    "def spectral_norm(pred, real):\n",
    "    \"\"\"Calculate difference in spectral norm between two batches of spectrograms.\"\"\"\n",
    "    norm_real = tf.norm(real, axis=(1,2)) + 1e-6\n",
    "    norm_pred = tf.norm(pred, axis=(1,2)) + 1e-6\n",
    "    return tf.reduce_mean(tf.abs(norm_real - norm_pred) / norm_real)\n",
    "\n",
    "def time_derivative(pred, real, window=1):\n",
    "    real_derivative = real[:, :-window, :, :] - real[:, window:, :, :]\n",
    "    pred_derivative = pred[:, :-window, :, :] - pred[:, window:, :, :]\n",
    "    return tf.reduce_mean(tf.keras.losses.MSE(real_derivative, pred_derivative))\n",
    "\n",
    "\n",
    "\n",
    "class DDIM(keras.Model):\n",
    "    \"\"\"DDIM model modified from this tutorial: https://keras.io/examples/generative/ddim/\"\"\"\n",
    "    \n",
    "    def __init__(self, widths, block_depth, attention=False, dim1=256, dim2=128):\n",
    "        super().__init__()\n",
    "\n",
    "        self.normalizer = layers.Normalization(axis=(2,3))\n",
    "        self.network = get_network(widths, block_depth, attention=attention, dim1=dim1, dim2=dim2)\n",
    "        self.ema_network = keras.models.clone_model(self.network)\n",
    "        self.spec_mod = 0\n",
    "        self.dx_mod = 0\n",
    "\n",
    "    def compile(self, **kwargs):\n",
    "        super().compile(**kwargs)\n",
    "\n",
    "        self.noise_loss_tracker = keras.metrics.Mean(name=\"n_loss\")\n",
    "        self.data_loss_tracker = keras.metrics.Mean(name=\"d_loss\")\n",
    "        \n",
    "        self.noise_spec_tracker = keras.metrics.Mean(name=\"n_spec\")\n",
    "        self.data_spec_tracker = keras.metrics.Mean(name=\"d_spec\")\n",
    "        \n",
    "        self.noise_dx_tracker = keras.metrics.Mean(name=\"n_dx\")\n",
    "        self.data_dx_tracker = keras.metrics.Mean(name=\"d_dx\")\n",
    "        \n",
    "        self.noise_total_tracker = keras.metrics.Mean(name=\"n_total\")\n",
    "        self.data_total_tracker = keras.metrics.Mean(name=\"d_total\")\n",
    "        self.spec_mod = 0\n",
    "        self.dx_mod = 0\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.noise_loss_tracker, \n",
    "            self.data_loss_tracker,\n",
    "            \n",
    "            self.noise_spec_tracker,\n",
    "            self.data_spec_tracker,\n",
    "            \n",
    "            self.noise_dx_tracker,\n",
    "            self.data_dx_tracker,\n",
    "            \n",
    "            self.noise_total_tracker,\n",
    "            self.data_total_tracker\n",
    "        ]\n",
    "    \n",
    "    def update_trackers(self, n_l, n_s, n_d, d_l, d_s, d_d):\n",
    "        \"\"\"Update all loss trackers.\"\"\"\n",
    "        n_t = n_l + n_s + n_d\n",
    "        d_t = d_l + d_s + d_d\n",
    "        \n",
    "        for loss, tracker in zip(\n",
    "            [n_l, n_s, n_d, n_t, d_l, d_s, d_d, d_t], \n",
    "            [\n",
    "                self.noise_loss_tracker, self.noise_spec_tracker, self.noise_dx_tracker, self.noise_total_tracker,\n",
    "                self.data_loss_tracker, self.data_spec_tracker, self.data_dx_tracker, self.data_total_tracker\n",
    "            ]\n",
    "        ):\n",
    "            tracker.update_state(loss)\n",
    "            \n",
    "    def get_losses(self, y_true, y_pred):\n",
    "        \"\"\"Get losses for model.\"\"\"\n",
    "        return (\n",
    "            tf.reduce_mean(\n",
    "                self.loss(y_pred, y_true)\n",
    "            ), spectral_norm(\n",
    "                y_pred, y_true\n",
    "            ), time_derivative(\n",
    "                y_pred, y_true\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def denormalize(self, data):\n",
    "        data = self.normalizer.mean + data * self.normalizer.variance**0.5\n",
    "        return tf.clip_by_value(data, -128.0, 128.0)\n",
    "\n",
    "    def diffusion_schedule(self, diffusion_times):\n",
    "        start_angle = tf.acos(max_signal_rate)\n",
    "        end_angle = tf.acos(min_signal_rate)\n",
    "        diffusion_angles = start_angle + diffusion_times * (end_angle - start_angle)\n",
    "        signal_rates = tf.cos(diffusion_angles)\n",
    "        noise_rates = tf.sin(diffusion_angles)\n",
    "        return noise_rates, signal_rates\n",
    "\n",
    "    def denoise(self, noisy_data, noise_rates, signal_rates, training):\n",
    "        if training:\n",
    "            network = self.network\n",
    "        else:\n",
    "            network = self.ema_network\n",
    "        pred_noises = network([noisy_data, noise_rates**2], training=training)\n",
    "        pred_data = (noisy_data - noise_rates * pred_noises) / signal_rates\n",
    "\n",
    "        return pred_noises, pred_data\n",
    "\n",
    "    def reverse_diffusion(self, initial_noise, diffusion_steps):\n",
    "        num_examples = tf.shape(initial_noise)[0]\n",
    "        step_size = 1.0 / diffusion_steps\n",
    "\n",
    "        # important line:\n",
    "        # at the first sampling step, the \"noisy data\" is pure noise\n",
    "        # but its signal rate is assumed to be nonzero (min_signal_rate)\n",
    "        next_noisy_data = initial_noise\n",
    "        for step in tqdm(range(diffusion_steps)):\n",
    "            noisy_data = next_noisy_data\n",
    "\n",
    "            # separate the current noisy data to its components\n",
    "            diffusion_times = tf.ones((num_examples, 1, 1, 1)) - step * step_size\n",
    "            noise_rates, signal_rates = self.diffusion_schedule(diffusion_times)\n",
    "            pred_noises, pred_data = self.denoise(\n",
    "                noisy_data, noise_rates, signal_rates, training=False\n",
    "            )\n",
    "            # network used in eval mode\n",
    "\n",
    "            # remix the predicted components using the next signal and noise rates\n",
    "            next_diffusion_times = diffusion_times - step_size\n",
    "            next_noise_rates, next_signal_rates = self.diffusion_schedule(\n",
    "                next_diffusion_times\n",
    "            )\n",
    "            next_noisy_data = (\n",
    "                next_signal_rates * pred_data + next_noise_rates * pred_noises\n",
    "            )\n",
    "            # this new noisy data will be used in the next step\n",
    "\n",
    "        return pred_data\n",
    "\n",
    "    def generate(self, num_examples, shape, diffusion_steps):\n",
    "        # noise -> data -> denormalized data\n",
    "        initial_noise = tf.random.normal(shape=(num_examples, shape[0], shape[1], shape[2]))\n",
    "        generated_data = self.reverse_diffusion(initial_noise, diffusion_steps)\n",
    "        generated_data = self.denormalize(generated_data)\n",
    "        return generated_data\n",
    "\n",
    "    def train_step(self, data):\n",
    "        batch_size = tf.shape(data)[0]\n",
    "        # normalize data to have standard deviation of 1, like the noises\n",
    "        data = self.normalizer(data, training=True)\n",
    "        noises = tf.random.normal(shape=tf.shape(data))\n",
    "\n",
    "        # sample uniform random diffusion times\n",
    "        diffusion_times = tf.random.uniform(\n",
    "            shape=(batch_size, 1, 1, 1), minval=0.0, maxval=1.0\n",
    "        )\n",
    "        noise_rates, signal_rates = self.diffusion_schedule(diffusion_times)\n",
    "        noise_rates = noise_rates\n",
    "        signal_rates = signal_rates\n",
    "        # mix the data with noises accordingly\n",
    "        noisy_data = signal_rates * data + noise_rates * noises\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # train the network to separate noisy data to their components\n",
    "            pred_noises, pred_data = self.denoise(\n",
    "                noisy_data, noise_rates, signal_rates, training=True\n",
    "            )\n",
    "\n",
    "            noise_loss, noise_spec, noise_dx = self.get_losses(noises, pred_noises) #safe_reduce_mean(self.loss(noises, pred_noises))  # used for training\n",
    "            total_noise_loss = tf.reduce_sum([\n",
    "                noise_loss, \n",
    "                self.spec_mod*noise_spec, \n",
    "                self.dx_mod*noise_dx\n",
    "            ])\n",
    "            data_loss, data_spec, data_dx = self.get_losses(data, pred_data) #safe_reduce_mean(self.loss(data, pred_data))  # only used as metric\n",
    "        gradients = tape.gradient(noise_loss, self.network.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.network.trainable_weights))\n",
    "\n",
    "        self.update_trackers(\n",
    "            noise_loss, noise_spec, noise_dx,\n",
    "            data_loss, data_spec, data_dx\n",
    "        )\n",
    "\n",
    "        # track the exponential moving averages of weights\n",
    "        for weight, ema_weight in zip(self.network.weights, self.ema_network.weights):\n",
    "            ema_weight.assign(ema * ema_weight + (1 - ema) * weight)\n",
    "\n",
    "        # KID is not measured during the training phase for computational efficiency\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    def test_step(self, data):\n",
    "        # normalize data to have standard deviation of 1, like the noises\n",
    "        batch_size = tf.shape(data)[0]\n",
    "        \n",
    "        data = self.normalizer(data, training=False)\n",
    "        noises = tf.random.normal(shape=tf.shape(data))\n",
    "\n",
    "        # sample uniform random diffusion times\n",
    "        diffusion_times = tf.random.uniform(\n",
    "            shape=(batch_size, 1, 1, 1), minval=0.0, maxval=1.0\n",
    "        )\n",
    "        noise_rates, signal_rates = self.diffusion_schedule(diffusion_times)\n",
    "        # mix the data with noises accordingly\n",
    "        noisy_data = signal_rates * data + noise_rates * noises\n",
    "\n",
    "        # use the network to separate noisy data to their components\n",
    "        pred_noises, pred_data = self.denoise(\n",
    "            noisy_data, noise_rates, signal_rates, training=False\n",
    "        )\n",
    "\n",
    "        noise_loss = self.loss(noises, pred_noises)\n",
    "        data_loss = self.loss(data, pred_data)\n",
    "\n",
    "        self.data_loss_tracker.update_state(data_loss)\n",
    "        self.noise_loss_tracker.update_state(noise_loss)\n",
    "\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d279dc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_at_interval(x, rate=10_000, feats=256, duration=3.3):\n",
    "    \"\"\"Load music from file at some offset. Return MDCT spectrogram of that data\"\"\"\n",
    "    file = x[0].numpy().decode()\n",
    "    idx = x[1].numpy()\n",
    "    audio, sr = librosa.load(file, duration=duration, sr=rate, offset=idx)\n",
    "    audio_fill = np.zeros(int(rate*duration), dtype=np.float32)\n",
    "    audio_fill[:len(audio)] = audio\n",
    "    spec = tf.signal.mdct(audio_fill, feats)\n",
    "    return spec\n",
    "\n",
    "def load_audio(x,y, rate=10_000, mdct_feats=256, duration=3.3):\n",
    "    \"\"\"TF function for loading MDCT spectrogram from file.\"\"\"\n",
    "    out = tf.py_function(lambda x,y: load_at_interval( \n",
    "        (x,y), rate=rate, feats=mdct_feats, duration=duration\n",
    "    ), inp=[x,y], Tout=tf.float32)\n",
    "    return out\n",
    "\n",
    "def get_files_dataset(\n",
    "        glob_location,\n",
    "        total_seconds=2,\n",
    "        out_len = 3.3,\n",
    "        hop_size=1,\n",
    "        max_feats = 2048,\n",
    "        batch_size=4,\n",
    "        shuffer_size=1000,\n",
    "        scale=1,\n",
    "        rate=10_000,\n",
    "        mdct_feats=256\n",
    "    ):\n",
    "    \"\"\"Get file dataset loader for a glob of audio files.\"\"\"\n",
    "    \n",
    "    files = glob(\n",
    "        glob_location,\n",
    "        recursive=True\n",
    "    )\n",
    "    \n",
    "#     files = [file for file in files if file not in exclude]\n",
    "    \n",
    "    def file_list_generator():\n",
    "        for _ in range(total_seconds):\n",
    "            for file in files:\n",
    "                yield file, _*hop_size\n",
    "                \n",
    "    load_fn = partial(load_audio, duration=out_len, rate=rate, mdct_feats=mdct_feats)\n",
    "                \n",
    "    dg =tf.data.Dataset.from_generator(file_list_generator, output_signature = (\n",
    "        tf.TensorSpec(shape=(), dtype=tf.string), \n",
    "        tf.TensorSpec(shape=(), dtype=tf.int32))).shuffle(shuffer_size).map(\n",
    "            load_fn, num_parallel_calls=tf.data.AUTOTUNE\n",
    "        ).map(\n",
    "            lambda x: tf.expand_dims(x, -1)[:max_feats, :, :]*scale\n",
    "        ).map(\n",
    "            lambda x: tf.ensure_shape(x, (max_feats, mdct_feats//2, 1))\n",
    "        ).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    return dg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e4898fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = get_files_dataset(\n",
    "    \"data/REAL_audio/**/*.wav\", \n",
    "    out_len=3.3, \n",
    "    max_feats=256, \n",
    "    total_seconds=26, \n",
    "    scale=1,\n",
    "    batch_size=16\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "55b245a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 256, 128, 1)\n"
     ]
    }
   ],
   "source": [
    "for test_batch in dataset.take(1):\n",
    "    shape = test_batch.shape\n",
    "\n",
    "print(shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6c047f06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1625"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_total_examples = (len(music_files) * 26) // shape[0]\n",
    "num_total_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "201d8e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def block(x, t_emb, ch, use_attn=False):\n",
    "    # First conv\n",
    "    h = layers.Conv2D(ch, 3, padding=\"same\")(x)\n",
    "    h = layers.GroupNormalization(groups=8)(h)\n",
    "    # FiLM-like conditioning with time embedding\n",
    "    gamma = layers.Dense(ch)(t_emb)\n",
    "    beta  = layers.Dense(ch)(t_emb)\n",
    "    h = layers.Add()([h, gamma])  # shift\n",
    "    h = layers.Activation(\"swish\")(h)\n",
    "    h = layers.Add()([h, beta])   # bias\n",
    "\n",
    "    # Second conv\n",
    "    h = layers.Conv2D(ch, 3, padding=\"same\")(h)\n",
    "    h = layers.GroupNormalization(groups=8)(h)\n",
    "    h = layers.Activation(\"swish\")(h)\n",
    "\n",
    "    # Optional lightweight self-attention\n",
    "    if use_attn:\n",
    "        q = layers.Conv2D(ch, 1, padding=\"same\")(h)\n",
    "        k = layers.Conv2D(ch, 1, padding=\"same\")(h)\n",
    "        v = layers.Conv2D(ch, 1, padding=\"same\")(h)\n",
    "        attn = layers.Softmax(axis=-1)(\n",
    "            layers.Lambda(lambda x: tf.matmul(\n",
    "                tf.reshape(x[0], [tf.shape(x[0])[0], -1, tf.shape(x[0])[-1]]),\n",
    "                tf.transpose(tf.reshape(x[1], [tf.shape(x[1])[0], -1, tf.shape(x[1])[-1]]), [0, 2, 1])\n",
    "            ))([q, k])\n",
    "        )\n",
    "        v_flat = layers.Lambda(lambda x: tf.reshape(x, [tf.shape(x)[0], -1, tf.shape(x)[-1]]))(v)\n",
    "        h_attn = layers.Lambda(lambda x: tf.reshape(\n",
    "            tf.matmul(x[0], x[1]),\n",
    "            [tf.shape(x[0])[0], tf.shape(h)[1], tf.shape(h)[2], tf.shape(h)[-1]]\n",
    "        ))([attn, v_flat])\n",
    "        h = layers.Add()([h, h_attn])\n",
    "\n",
    "    return h\n",
    "\n",
    "def time_embedding(t, emb_dim=128):\n",
    "    # t is shape (B,1,1,1). Flatten -> MLP -> (B, emb_dim)\n",
    "    x = layers.Flatten()(t)\n",
    "    x = layers.Dense(emb_dim, activation=\"swish\")(x)\n",
    "    x = layers.Dense(emb_dim, activation=\"swish\")(x)\n",
    "    # reshape to (B,1,1,emb_dim) so Dense layers above can broadcast\n",
    "    return layers.Reshape((1, 1, emb_dim))(x)\n",
    "\n",
    "def get_network(widths, block_depth, attention=False, dim1=256, dim2=128):\n",
    "    \"\"\"U-Net–ish noise predictor conditioned on diffusion time.\"\"\"\n",
    "    x_in = layers.Input(shape=(dim1, dim2, 1))\n",
    "    t_in = layers.Input(shape=(1, 1, 1))\n",
    "    t_emb = time_embedding(t_in, emb_dim=128)\n",
    "\n",
    "    # Down path\n",
    "    skips = []\n",
    "    x = x_in\n",
    "    for i, ch in enumerate(widths):\n",
    "        for _ in range(block_depth):\n",
    "            x = block(x, t_emb, ch, use_attn=attention and (i >= len(widths)//2))\n",
    "        skips.append(x)\n",
    "        if i < len(widths) - 1:\n",
    "            x = layers.Conv2D(ch, 3, strides=2, padding=\"same\")(x)\n",
    "\n",
    "    # Mid\n",
    "    x = block(x, t_emb, widths[-1], use_attn=attention)\n",
    "\n",
    "    # Up path\n",
    "    for i, ch in reversed(list(enumerate(widths[:-1]))):\n",
    "        x = layers.UpSampling2D()(x)\n",
    "        x = layers.Conv2D(ch, 3, padding=\"same\")(x)\n",
    "        x = layers.Concatenate()([x, skips[i]])\n",
    "        for _ in range(block_depth):\n",
    "            x = block(x, t_emb, ch, use_attn=attention and (i >= len(widths)//2))\n",
    "\n",
    "    # Output noise prediction\n",
    "    out = layers.Conv2D(1, 1, padding=\"same\")(x)\n",
    "    return keras.Model([x_in, t_in], out, name=\"ddim_unet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6e28bcb3",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Could not build a TypeSpec for KerasTensor(type_spec=TensorSpec(shape=(None, 64, 32, 128), dtype=tf.float32, name=None), name='tf.reshape/Reshape:0', description=\"created by layer 'tf.reshape'\") of unsupported type <class 'keras.src.engine.keras_tensor.KerasTensor'>.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mDDIM\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwidths\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock_depth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m             \u001b[49m\u001b[43mattention\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[13], line 25\u001b[0m, in \u001b[0;36mDDIM.__init__\u001b[1;34m(self, widths, block_depth, attention, dim1, dim2)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalizer \u001b[38;5;241m=\u001b[39m layers\u001b[38;5;241m.\u001b[39mNormalization(axis\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m))\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetwork \u001b[38;5;241m=\u001b[39m \u001b[43mget_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwidths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock_depth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdim1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdim2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mema_network \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mclone_model(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetwork)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspec_mod \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "Cell \u001b[1;32mIn[20], line 56\u001b[0m, in \u001b[0;36mget_network\u001b[1;34m(widths, block_depth, attention, dim1, dim2)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, ch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(widths):\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(block_depth):\n\u001b[1;32m---> 56\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_emb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_attn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mwidths\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m     skips\u001b[38;5;241m.\u001b[39mappend(x)\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(widths) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "Cell \u001b[1;32mIn[20], line 29\u001b[0m, in \u001b[0;36mblock\u001b[1;34m(x, t_emb, ch, use_attn)\u001b[0m\n\u001b[0;32m     22\u001b[0m     attn \u001b[38;5;241m=\u001b[39m layers\u001b[38;5;241m.\u001b[39mSoftmax(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)(\n\u001b[0;32m     23\u001b[0m         layers\u001b[38;5;241m.\u001b[39mLambda(\u001b[38;5;28;01mlambda\u001b[39;00m x: tf\u001b[38;5;241m.\u001b[39mmatmul(\n\u001b[0;32m     24\u001b[0m             tf\u001b[38;5;241m.\u001b[39mreshape(x[\u001b[38;5;241m0\u001b[39m], [tf\u001b[38;5;241m.\u001b[39mshape(x[\u001b[38;5;241m0\u001b[39m])[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, tf\u001b[38;5;241m.\u001b[39mshape(x[\u001b[38;5;241m0\u001b[39m])[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]]),\n\u001b[0;32m     25\u001b[0m             tf\u001b[38;5;241m.\u001b[39mtranspose(tf\u001b[38;5;241m.\u001b[39mreshape(x[\u001b[38;5;241m1\u001b[39m], [tf\u001b[38;5;241m.\u001b[39mshape(x[\u001b[38;5;241m1\u001b[39m])[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, tf\u001b[38;5;241m.\u001b[39mshape(x[\u001b[38;5;241m1\u001b[39m])[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]]), [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m     26\u001b[0m         ))([q, k])\n\u001b[0;32m     27\u001b[0m     )\n\u001b[0;32m     28\u001b[0m     v_flat \u001b[38;5;241m=\u001b[39m layers\u001b[38;5;241m.\u001b[39mLambda(\u001b[38;5;28;01mlambda\u001b[39;00m x: tf\u001b[38;5;241m.\u001b[39mreshape(x, [tf\u001b[38;5;241m.\u001b[39mshape(x)[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, tf\u001b[38;5;241m.\u001b[39mshape(x)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]]))(v)\n\u001b[1;32m---> 29\u001b[0m     h_attn \u001b[38;5;241m=\u001b[39m \u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLambda\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mattn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv_flat\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m     h \u001b[38;5;241m=\u001b[39m layers\u001b[38;5;241m.\u001b[39mAdd()([h, h_attn])\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m h\n",
      "File \u001b[1;32mc:\\Users\\arthu\\Documents\\GAN_Music_Generator\\venv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\arthu\\Documents\\GAN_Music_Generator\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\type_spec.py:962\u001b[0m, in \u001b[0;36mtype_spec_from_value\u001b[1;34m(value)\u001b[0m\n\u001b[0;32m    958\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    959\u001b[0m   logging\u001b[38;5;241m.\u001b[39mvlog(\n\u001b[0;32m    960\u001b[0m       \u001b[38;5;241m3\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to convert \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m to tensor: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mtype\u001b[39m(value)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, e))\n\u001b[1;32m--> 962\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not build a TypeSpec for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    963\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munsupported type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(value)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: Could not build a TypeSpec for KerasTensor(type_spec=TensorSpec(shape=(None, 64, 32, 128), dtype=tf.float32, name=None), name='tf.reshape/Reshape:0', description=\"created by layer 'tf.reshape'\") of unsupported type <class 'keras.src.engine.keras_tensor.KerasTensor'>."
     ]
    }
   ],
   "source": [
    "model = DDIM(widths = [128, 128, 128, 128], block_depth = 2, \n",
    "             attention=True, dim1=shape[1], dim2=shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d39f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.normalizer.adapt(dataset, steps=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592f4552",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss=tf.keras.losses.MSE,\n",
    "    optimizer= tfa.optimizers.AdamW(\n",
    "        learning_rate = 3e-4,\n",
    "        weight_decay = 1e-4\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15172271",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3ee432",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(dataset.repeat(), steps_per_epoch=num_total_examples, epochs=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f460d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.spec_mod = 1\n",
    "model.dx_mod = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963c92ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(dataset.repeat(), steps_per_epoch=num_total_examples, epochs=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b441c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "specs = model.generate(8, shape[1:], 1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4175e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    plt.pcolormesh(np.log(np.abs(test_batch[i, :, :, 0].numpy().T)))\n",
    "    plt.colorbar()\n",
    "    plt.title(f\"Real example {i+1}\")\n",
    "    plt.show()\n",
    "    ipd.display(ipd.Audio(tf.signal.inverse_mdct(test_batch[i, :, :, 0]), rate=10_000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85e4f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(specs)):\n",
    "    plt.pcolormesh(np.log(np.abs(specs[i, :, :, 0].numpy().T)))\n",
    "    plt.colorbar()\n",
    "    plt.title(f\"Generated example {i+1}\")\n",
    "    plt.show()\n",
    "    ipd.display(ipd.Audio(tf.signal.inverse_mdct(tf.cast(specs[i, :, :, 0], tf.float32)), rate=10_000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5914139c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
